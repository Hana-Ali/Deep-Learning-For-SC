{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "from training import *\n",
    "\n",
    "hpc = False\n",
    "labs = False\n",
    "paperspace = True\n",
    "\n",
    "if hpc:\n",
    "    main_data_path = \"/rds/general/user/hsa22/ephemeral/Brain_MINDS/model_data\"\n",
    "    main_logs_path = \"/rds/general/user/hsa22/ephemeral/Brain_MINDS/predicted_streamlines\"\n",
    "elif labs:\n",
    "    main_data_path = \"/media/hsa22/Expansion/Brain_MINDS/model_data\"\n",
    "    main_logs_path = \"/media/hsa22/Expansion//Brain_MINDS/predicted_streamlines\"\n",
    "elif paperspace:\n",
    "    main_data_path = \"/notebooks/model_data_w_resize\"\n",
    "    main_logs_path = \"/notebooks/predicted_streamlines\"\n",
    "else:\n",
    "    main_data_path = \"D:\\\\Brain-MINDS\\\\model_data\"\n",
    "    main_logs_path = \"D:\\\\Brain-MINDS\\\\predicted_streamlines\"\n",
    "\n",
    "streamline_arrays_path = os.path.join(main_logs_path, \"streamline_predictions\", \"resnet_streamlines\")\n",
    "training_log_folder = os.path.join(main_logs_path, \"training_logs\")\n",
    "model_folder = os.path.join(main_logs_path, \"models\", \"resnet_streamlines\")\n",
    "\n",
    "check_output_folders(streamline_arrays_path, \"streamline arrays\", wipe=False)\n",
    "check_output_folders(training_log_folder, \"training_log_folder\", wipe=False)\n",
    "check_output_folders(model_folder, \"model_folder\", wipe=False)\n",
    "\n",
    "training_log_path = os.path.join(training_log_folder, \"resnet_streamlines.csv\")\n",
    "model_filename = os.path.join(model_folder, \"resnet_streamlines.h5\")\n",
    "\n",
    "# Create the configs dictionary\n",
    "config = {\n",
    "\n",
    "    ####### Model #######\n",
    "    \"model_name\" : \"resnet_streamlines\", # Model name\n",
    "    \"input_nc\" : 1,\n",
    "    \"combination\" : True, # Combination\n",
    "    \"task\" : \"classification\", # Task\n",
    "    \"hidden_size\" : 32, # number of neurons\n",
    "    \"depthwise_conv\" : True, # Depthwise convolution\n",
    "    \"library_opt\" : True, # Use stuff from torch_optim\n",
    "    \"contrastive\" : \"npair\", # Contrastive\n",
    "\n",
    "    ####### Training #######\n",
    "    \"n_epochs\" : 50, # Number of epochs\n",
    "    \"loss\" : \"negative_log_likelihood_loss\", # Loss function\n",
    "    \"optimizer\" : \"Adam\", # Optimizer\n",
    "    \"evaluation_metric\" : \"negative_log_likelihood_loss\", # Evaluation metric\n",
    "    \"shuffle_dataset\" : True,\n",
    "    \"separate_hemisphere\" : False,\n",
    "    \"cube_size\" : 5, # cube size\n",
    "    \"save_best\" : True, # Save best model\n",
    "    \"overfitting\" : False, # Overfitting\n",
    "\n",
    "    ####### Data #######\n",
    "    \"main_data_path\" : main_data_path, # Data path\n",
    "    \"training_log_path\" : training_log_path, # Training log path\n",
    "    \"model_filename\" : model_filename, # Model filename\n",
    "    \"streamline_arrays_path\" : streamline_arrays_path, # Path to the streamlines array\n",
    "    \"batch_size\" : 32, # Batch size\n",
    "    \"validation_batch_size\" : 32, # Validation batch size\n",
    "    \"num_streamlines\" : 70, # Number of streamlines to consider from each site\n",
    "    \n",
    "    ####### Parameters #######\n",
    "    \"initial_learning_rate\" : 0.05, # Initial learning rate\n",
    "    \"early_stopping_patience\": 50, # Early stopping patience\n",
    "    \"decay_patience\": 20, # Learning rate decay patience\n",
    "    \"decay_factor\": 0.5, # Learning rate decay factor\n",
    "    \"min_learning_rate\": 1e-08, # Minimum learning rate\n",
    "    \"save_last_n_models\": 10, # Save last n models\n",
    "\n",
    "    ####### Misc #######\n",
    "    \"skip_val\" : False, # Skip validation\n",
    "    \"training_type\" : \"streamline\", # Training type\n",
    "    \"tck_type\" : \"trk\" # TCK type\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model information\n",
    "model_name = config[\"model_name\"]\n",
    "model_filename = config[\"model_filename\"]\n",
    "main_data_path = config[\"main_data_path\"]\n",
    "training_log_path = config[\"training_log_path\"]\n",
    "residual_arrays_path = config[\"residual_arrays_path\"] if \"residual_arrays_path\" in config else None\n",
    "streamline_arrays_path = config[\"streamline_arrays_path\"] if \"streamline_arrays_path\" in config else None\n",
    "\n",
    "# Get the training parameters\n",
    "n_epochs = config[\"n_epochs\"]\n",
    "learning_rate_decay_patience = config[\"decay_patience\"] if \"decay_patience\" in config else None\n",
    "learning_rate_decay_step_size = config[\"decay_step_size\"] if \"decay_step_size\" in config else None\n",
    "decay_factor = config[\"decay_factor\"] if \"decay_factor\" in config else 0.1\n",
    "min_lr = config[\"min_learning_rate\"] if \"min_learning_rate\" in config else 0.\n",
    "early_stopping_patience = config[\"early_stopping_patience\"] if \"early_stopping_patience\" in config else None\n",
    "separate_hemisphere = config[\"separate_hemisphere\"] if \"separate_hemisphere\" in config else True\n",
    "voxel_wise = config[\"voxel_wise\"] if \"voxel_wise\" in config else False\n",
    "cube_size = config[\"cube_size\"] if \"cube_size\" in config else 5\n",
    "\n",
    "# Get general parameters\n",
    "n_gpus = config[\"n_gpus\"] if \"n_gpus\" in config else 1\n",
    "n_workers = config[\"n_workers\"] if \"n_workers\" in config else 1\n",
    "pin_memory = config[\"pin_memory\"] if \"pin_memory\" in config else False\n",
    "prefetch_factor = config[\"prefetch_factor\"] if \"prefetch_factor\" in config else 1\n",
    "amp = config[\"amp\"] if \"amp\" in config else False\n",
    "save_best = config[\"save_best\"] if \"save_best\" in config else False\n",
    "save_every_n_epochs = config[\"save_every_n_epochs\"] if \"save_every_n_epochs\" in config else None\n",
    "save_last_n_models = config[\"save_last_n_models\"] if \"save_last_n_models\" in config else None\n",
    "verbose = config[\"verbose\"] if \"verbose\" in config else 1\n",
    "\n",
    "# Define the output size depending on the task\n",
    "if in_config(\"task\", config, None) == \"classification\" and not in_config(\"contrastive\", config, None):\n",
    "    output_size = 27 # Predicting directions, there are 27 bins\n",
    "elif in_config(\"task\", config, None) == \"regression_angles\" and not in_config(\"contrastive\", config, None):\n",
    "    output_size = 3 # Predicting angles\n",
    "elif in_config(\"task\", config, None) == \"regression_coords\" and not in_config(\"contrastive\", config, None):\n",
    "    output_size = 3 # Predicting coordinates\n",
    "elif in_config(\"contrastive\", config, None):\n",
    "    output_size = 256 # Predicting contrastive loss\n",
    "    \n",
    "# Build or load the model depending on streamline or dwi training, and build dataset differently\n",
    "if config[\"training_type\"] == \"streamline\":\n",
    "    # Build the model\n",
    "    model = build_or_load_model(model_name, model_filename, input_nc=config[\"input_nc\"], cube_size=config[\"cube_size\"],\n",
    "                                num_rnn_layers=in_config(\"num_rnn_layers\", config, None), num_rnn_hidden_neurons=in_config(\"num_rnn_hidden_neurons\", config, None),\n",
    "                                num_nodes=in_config(\"num_nodes\", config, None), num_coordinates=in_config(\"num_coordinates\", config, None),\n",
    "                                prev_output_size=in_config(\"prev_output_size\", config, False), combination=config[\"combination\"],\n",
    "                                n_gpus=n_gpus, bias=bias, freeze_bias=in_config(\"freeze_bias\", config, False),\n",
    "                                strict=False, task=in_config(\"task\", config, \"classification\"), output_size=output_size,\n",
    "                                hidden_size=in_config(\"hidden_size\", config, 128), batch_norm=True if config[\"batch_size\"] > 1 else False,\n",
    "                                depthwise_conv=in_config(\"depthwise_conv\", config, False), contrastive=in_config(\"contrastive\", config, False))\n",
    "    # Build the dataset\n",
    "    dataset = StreamlineDataset(main_data_path, num_streamlines=config[\"num_streamlines\"], transforms=None, train=True, tck_type=config[\"tck_type\"], \n",
    "                                task=in_config(\"task\", config, \"classification\"))\n",
    "elif config[\"training_type\"] == \"residual\":\n",
    "    # Build the model\n",
    "    model = build_or_load_model(model_name, model_filename, input_nc=config[\"input_nc\"], \n",
    "                                output_nc=config[\"output_nc\"], ngf=config[\"ngf\"], \n",
    "                                num_blocks=config[\"num_blocks\"], norm_layer=config[\"norm_layer\"],\n",
    "                                use_dropout=config[\"use_dropout\"], padding_type=config[\"padding_type\"],\n",
    "                                cube_size=config[\"cube_size\"],\n",
    "                                n_gpus=n_gpus, bias=bias, freeze_bias=in_config(\"freeze_bias\", config, False),\n",
    "                                strict=False, voxel_wise=config[\"voxel_wise\"])\n",
    "    # Build the dataset\n",
    "    dataset = NiftiDataset(main_data_path, transforms=None, train=True)\n",
    "else:\n",
    "    raise ValueError(\"Training type {} not found\".format(config[\"training_type\"]))\n",
    "    \n",
    "# Print the model name and metric to monitor as logging\n",
    "print(\"Model is: {}\".format(model.__class__.__name__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If given a task, then get a specific criterion\n",
    "if in_config(\"contrastive\", config, None):\n",
    "    if config[\"contrastive\"] == \"max_margin\":\n",
    "        criterion = ContrastiveLossWithPosNegPairs()\n",
    "    elif config[\"contrastive\"] == \"npair\":\n",
    "        criterion = MultiClassNPairLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Contrastive loss {} not found\".format(config[\"contrastive\"]))\n",
    "else:\n",
    "    if in_config(\"task\", config, None) == \"classification\":\n",
    "        criterion = negative_log_likelihood_loss\n",
    "    elif in_config(\"task\", config, None) == \"regression_angles\" or in_config(\"task\", config, None) == \"regression_coords\":\n",
    "        criterion = MSE_loss\n",
    "    else: # If no task is given, then we need to load one according to the evaluation metric\n",
    "        criterion = load_criterion(config['evaluation_metric'], n_gpus=n_gpus)\n",
    "\n",
    "# If weighted loss\n",
    "if \"weights\" in config and config[\"weights\"] is not None:\n",
    "    criterion = loss_funcs.WeightedLoss(torch.tensor(config[\"weights\"]), criterion)\n",
    "    \n",
    "print(\"Criterion is: \", criterion)\n",
    "\n",
    "# Define the optimizer IF WE'RE NOT USING THE LIBRARY\n",
    "if in_config(\"library_opt\", config, None) != None:\n",
    "    # Using MADGRAD\n",
    "    optimizer = optim.MADGRAD(\n",
    "    model.parameters(),\n",
    "    lr=config[\"initial_learning_rate\"],\n",
    "    momentum=0.9,\n",
    "    weight_decay=config[\"decay_factor\"] if in_config(\"decay_factor\", config, None) != None else 0,\n",
    "    eps=1e-6,\n",
    "    )\n",
    "else:\n",
    "    # Optimizer kwargs dictionary\n",
    "    optimizer_kwargs = dict()\n",
    "\n",
    "    # If initial learning rate in config\n",
    "    if \"initial_learning_rate\" in config:\n",
    "        optimizer_kwargs[\"learning_rate\"] = config[\"initial_learning_rate\"]\n",
    "\n",
    "    # Build the optimizer\n",
    "    optimizer = build_optimizer(optimizer_name=config[\"optimizer\"],\n",
    "                                model_parameters=model.parameters(),\n",
    "                                **optimizer_kwargs)\n",
    "\n",
    "print(\"Optimizer is\", optimizer)\n",
    "# Get default collate\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "collate_fn = default_collate\n",
    "\n",
    "# Define the split size\n",
    "proportions = [.75, .10, .15]\n",
    "lengths = [int(p * len(dataset)) for p in proportions]\n",
    "lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
    "\n",
    "# Split the data\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, lengths, generator=seed)\n",
    "\n",
    "# Define the training loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                            batch_size=config[\"batch_size\"],\n",
    "                                            shuffle=config[\"shuffle_dataset\"],\n",
    "                                            num_workers=n_workers,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            prefetch_factor=prefetch_factor)    \n",
    "\n",
    "# Define the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=config[\"batch_size\"],\n",
    "                                        shuffle=config[\"shuffle_dataset\"],\n",
    "                                        num_workers=n_workers,\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        pin_memory=pin_memory,\n",
    "                                        prefetch_factor=prefetch_factor)\n",
    "    \n",
    "# Define the test loader\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                            batch_size=config[\"batch_size\"],\n",
    "                                            shuffle=config[\"shuffle_dataset\"],\n",
    "                                            num_workers=n_workers,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            prefetch_factor=prefetch_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the train loader\n",
    "(wmfod, streamlines, labels) = next(iter(train_loader))\n",
    "\n",
    "print(\"wmfod.shape:\", wmfod.shape)\n",
    "print(\"streamlines.shape:\", streamlines.shape)\n",
    "print(\"labels.shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the best weights\n",
    "best_weights_path = \"/notebooks/predicted_streamlines/models/resnet_streamlines/resnet_streamlines_best.h5\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
