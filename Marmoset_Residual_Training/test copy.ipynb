{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 128, 178, 115])\n",
      "torch.Size([16, 1, 128, 178, 115])\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "random_b0 = torch.rand((16, 1, 128, 178, 115))\n",
    "random_residual = torch.rand((16, 1, 128, 178, 115))\n",
    "\n",
    "print(random_b0.shape)\n",
    "print(random_residual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m b0_cube \u001b[39m=\u001b[39m grab_cube_around_voxel(image\u001b[39m=\u001b[39mrandom_b0, voxel_coordinates\u001b[39m=\u001b[39m[\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m      4\u001b[0m b0_cube \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(b0_cube)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from training import *\n",
    "\n",
    "b0_cube = grab_cube_around_voxel(image=random_b0, voxel_coordinates=[5, 5, 5], kernel_size=16)\n",
    "b0_cube = torch.from_numpy(b0_cube).float()\n",
    "\n",
    "res_cube = grab_cube_around_voxel(image=random_residual, voxel_coordinates=[5, 5, 5], kernel_size=8)\n",
    "res_cube = torch.from_numpy(res_cube).float()\n",
    "\n",
    "injection_center = torch.rand((16, 1, 1, 1, 3))\n",
    "image_coordinates = torch.rand((16, 1, 1, 1, 3))\n",
    "\n",
    "print(b0_cube.shape)\n",
    "print(res_cube.shape)\n",
    "print(injection_center.shape)\n",
    "print(image_coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResnetEncoder(nn.Module):\n",
    "    \n",
    "    # Constructor \n",
    "    def __init__(self, input_nc, output_nc=1, ngf=64, n_blocks=6, norm_layer=nn.BatchNorm3d, use_dropout=False, \n",
    "                 padding_type='reflect', voxel_wise=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_nc (int) -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            ngf (int) -- the number of filters in the last conv layer\n",
    "            n_blocks (int) -- the number of residual blocks\n",
    "            norm_layer -- normalization layer\n",
    "            use_dropout (bool) -- if use dropout layers\n",
    "            padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize parent class\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        # Initialize the self attributes\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.n_blocks = n_blocks\n",
    "        self.norm_layer = norm_layer\n",
    "        self.use_dropout = use_dropout\n",
    "        self.padding_type = padding_type\n",
    "        self.voxel_wise = voxel_wise\n",
    "\n",
    "        # Whatever this is\n",
    "        if type(norm_layer) == partial:\n",
    "            self.use_bias = norm_layer.func == nn.InstanceNorm3d\n",
    "        else:\n",
    "            self.use_bias = norm_layer == nn.InstanceNorm3d\n",
    "\n",
    "        # Define the models\n",
    "        self.img_model = self.define_img_model()\n",
    "        self.non_img_model = self.define_non_img_model()\n",
    "        self.joint_model = self.define_joint_model()\n",
    "\n",
    "    # Define the model\n",
    "    def define_img_model(self):\n",
    "        \"\"\"\n",
    "        Define the model architecture\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the model and padding size\n",
    "        model = []\n",
    "        padding_size = 3\n",
    "        \n",
    "        # Define the stride, based on voxel_wise\n",
    "        if self.voxel_wise:\n",
    "            stride = 2\n",
    "        else:\n",
    "            stride = 1\n",
    "\n",
    "        # Define the padding layer\n",
    "        if self.padding_type == 'reflect':\n",
    "            padding_layer = nn.ReflectionPad3d(padding_size)\n",
    "        elif self.padding_type == 'replicate':\n",
    "            padding_layer = nn.ReplicationPad3d(padding_size)\n",
    "        elif self.padding_type == 'zero':\n",
    "            padding_layer = nn.ZeroPad3d(padding_size)\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % self.padding_type)\n",
    "        \n",
    "        # 1. Add the first block\n",
    "        model.extend([padding_layer, \n",
    "                      nn.Conv3d(self.input_nc, self.ngf, kernel_size=7, padding=0, bias=self.use_bias), \n",
    "                      self.norm_layer(self.ngf), nn.ReLU(True)])\n",
    "        \n",
    "        # 2. Add one convolution\n",
    "        number_downsampling = 2\n",
    "        self.number_downsampling = number_downsampling\n",
    "        mult = 2**number_downsampling\n",
    "        model += [nn.Conv3d(self.ngf, self.ngf * mult, kernel_size=3,\n",
    "                    stride=1, padding=1, bias=False),\n",
    "                        self.norm_layer(self.ngf * mult), nn.ReLU(True)]\n",
    "\n",
    "        # 3. Add the residual blocks\n",
    "        for i in range(self.n_blocks):\n",
    "            model += [ResnetBlock(self.ngf * mult, padding_type=self.padding_type, \n",
    "                                  norm_layer=self.norm_layer, use_dropout=self.use_dropout, \n",
    "                                  use_bias=self.use_bias)]\n",
    "            \n",
    "        # 4. Add more downsampling blocks\n",
    "        # Cube output: stride 1 | Voxel output: stride 2\n",
    "        for i in range(number_downsampling):\n",
    "            mult = 2**(number_downsampling - i)\n",
    "            model += [nn.Conv3d(self.ngf * mult, int(self.ngf * mult / 2), \n",
    "                                kernel_size=3, stride=stride, padding=1, bias=self.use_bias), \n",
    "                          self.norm_layer(int(self.ngf * mult / 2)), \n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        # 5. Add another convolutional block for vibes\n",
    "        # Cube output: stride 1 | Voxel output: stride 2\n",
    "        model += [nn.Conv3d(int(self.ngf * mult / 2), int(self.ngf * mult / 4),\n",
    "                            kernel_size=3, stride=stride, padding=1, bias=self.use_bias),\n",
    "                             self.norm_layer(int(self.ngf * mult / 4)), nn.ReLU(True)]\n",
    "            \n",
    "        # 4. Add the last block to make the number of channels as the output_nc and reduce spatial space\n",
    "        model += [nn.Conv3d(int(self.ngf * mult / 4), self.output_nc, kernel_size=3, stride=2, padding=1, bias=self.use_bias)]\n",
    "        \n",
    "        # Cube output: No Adaptive layer | Voxel output: Adaptive layer\n",
    "        if self.voxel_wise:\n",
    "            model += [nn.AdaptiveAvgPool3d((1, 1, 1))]\n",
    "        \n",
    "        # Return the model\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "    # Define the processing for the non-image inputs\n",
    "    def define_non_img_model(self):\n",
    "        \n",
    "        # Stores the model\n",
    "        model = []\n",
    "        \n",
    "        # Add convolutions for the injection centers and image coordinates - expected to have self.output_nc channels\n",
    "        for i in range(self.number_downsampling):\n",
    "            model += [nn.Conv3d(self.output_nc, self.output_nc, kernel_size=3, stride=1, padding=1, bias=self.use_bias),\n",
    "                      self.norm_layer(self.output_nc), \n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        # Return the model\n",
    "        return nn.Sequential(*model)\n",
    "            \n",
    "    # Define joint processing for everything\n",
    "    def define_joint_model(self):\n",
    "        \n",
    "        # Stores the model\n",
    "        model = []\n",
    "        \n",
    "        # Define the factor we multiply by, based on voxel_wise\n",
    "        if self.voxel_wise:\n",
    "            factor = 1\n",
    "        else:\n",
    "            factor = 3\n",
    "        \n",
    "        # Add final convolutions for image and non-image data\n",
    "        # Cube output: self.output_nc * 3 channels | Voxel output: self.output_nc channels\n",
    "        for i in range(self.number_downsampling):\n",
    "            model += [nn.Conv3d(self.output_nc * factor, self.output_nc * factor, kernel_size=3, stride=1, padding=1, \n",
    "                                bias=self.use_bias),\n",
    "                      self.norm_layer(self.output_nc * factor), \n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        # Final convolution to make the number of channels 1\n",
    "        # Cube output: self.output_nc * 3 channels | Voxel output: self.output_nc channels\n",
    "        model += [nn.Conv3d(self.output_nc * factor, self.output_nc, kernel_size=3, stride=1, padding=1, bias=self.use_bias)]\n",
    "        \n",
    "        # Cube output: No Adaptive layer | Voxel output: Adaptive layer\n",
    "        if self.voxel_wise:\n",
    "            model += [nn.AdaptiveAvgPool3d((1, 1, 1))]\n",
    "            \n",
    "        # Return the model\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "    # Get the normalization layer\n",
    "    def get_norm_layer(self, norm_layer):\n",
    "\n",
    "        # If the norm layer is batch norm, we return it\n",
    "        if \"batch\" in norm_layer.lower():\n",
    "            return nn.BatchNorm3d\n",
    "        elif \"instance\" in norm_layer.lower():\n",
    "            return nn.InstanceNorm3d\n",
    "        else:\n",
    "            raise NotImplementedError('normalization layer [%s] is not found' % norm_layer)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, input_x, injection_center, image_coordinates):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the dimension we concatenate along, depending on voxel wise\n",
    "        if self.voxel_wise:\n",
    "            dim = 4\n",
    "        else:\n",
    "            dim = 1\n",
    "\n",
    "        # Do all the convolutions on the cube first\n",
    "        for layer in self.img_model:\n",
    "            input_x = layer(input_x)\n",
    "            print(input_x.shape)\n",
    "            \n",
    "        # Do the convolutional layers for the injection center\n",
    "        injection_center = self.non_img_model(injection_center)\n",
    "        \n",
    "        # Do the convolutional layers for the image coordinates\n",
    "        image_coordinates = self.non_img_model(image_coordinates)\n",
    "        \n",
    "        # Concatenate the data along the number of channels\n",
    "        # Cube output: Dimension 1 | Voxel output: Dimension 4\n",
    "        input_x = torch.cat((input_x, injection_center), dim=dim)\n",
    "        input_x = torch.cat((input_x, image_coordinates), dim=dim)\n",
    "        \n",
    "        # Do the joint processing\n",
    "        joint_data = self.joint_model(input_x)\n",
    "                        \n",
    "        # Return the model\n",
    "        return joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import *\n",
    "\n",
    "###############################################################\n",
    "##################### Myronenko Conv Block #####################\n",
    "###############################################################\n",
    "class MyronenkoConvolutionBlock(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, norm_layer=None, norm_groups=8):\n",
    "        super(MyronenkoConvolutionBlock, self).__init__()\n",
    "        self.myronenko_block = self.build_myronenko(in_channels, out_channels, kernel_size, stride, padding, norm_layer, norm_groups)\n",
    "\n",
    "    # Build the Myronenko block\n",
    "    def build_myronenko(self, in_channels, out_channels, kernel_size, stride, padding, norm_layer, norm_groups):\n",
    "\n",
    "        # If norm layer is not specified, then we use Group norm\n",
    "        if norm_layer is None:\n",
    "            self.norm_layer = nn.GroupNorm\n",
    "        else:\n",
    "            self.norm_layer = norm_layer\n",
    "\n",
    "        # Set the number of norm groups\n",
    "        self.norm_groups = norm_groups\n",
    "        \n",
    "        # This will hold the Myronenko block\n",
    "        myronenko_block = []\n",
    "\n",
    "        # 1. Add the norm layer\n",
    "        myronenko_block += [self.create_norm_layer(in_channels)]\n",
    "\n",
    "        # 2. Add the ReLU layer\n",
    "        myronenko_block += [nn.ReLU(True)]\n",
    "\n",
    "        # 3. Add the convolutional layer\n",
    "        myronenko_block += [nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)]\n",
    "\n",
    "        return nn.Sequential(*myronenko_block)\n",
    "    \n",
    "    # Create the normalization layer\n",
    "    def create_norm_layer(self, in_channels, error_on_non_divisible_norm_groups=False):\n",
    "\n",
    "        # If the number of in channels is less than the number of norm groups, then we use instance norm\n",
    "        if in_channels < self.norm_groups:\n",
    "            return self.norm_layer(in_channels, in_channels)\n",
    "        # If the number of in channels is divisible by the number of norm groups, then we use group norm\n",
    "        elif not error_on_non_divisible_norm_groups and (in_channels % self.norm_groups) > 0:\n",
    "            print(\"Setting number of norm groups to {} for this convolution block.\".format(in_channels))\n",
    "            return self.norm_layer(in_channels, in_channels)\n",
    "        # Otherwise, we use group norm\n",
    "        else:\n",
    "            return self.norm_layer(self.norm_groups, in_channels)\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, x_input):\n",
    "        for layer in self.myronenko_block:\n",
    "            x_input = layer(x_input)\n",
    "            print(\"Shape of input in MyronenkoConvBlock: \", x_input.shape)\n",
    "\n",
    "        return x_input\n",
    "    \n",
    "###############################################################\n",
    "####################### Myronenko Block #######################\n",
    "###############################################################\n",
    "class MyronenkoResidualBlock(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None, norm_groups=8):\n",
    "        super(MyronenkoResidualBlock, self).__init__()\n",
    "        self.myronenko_block = self.build_myronenko_block(in_channels, out_channels, kernel_size, stride, padding, norm_layer, norm_groups)\n",
    "\n",
    "    # Build the Myronenko block\n",
    "    def build_myronenko_block(self, in_channels, out_channels, kernel_size, stride, padding, norm_layer, norm_groups):\n",
    "\n",
    "        # Define the first convolutional block\n",
    "        conv_1 = MyronenkoConvolutionBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, \n",
    "                                           padding=padding, norm_layer=norm_layer, norm_groups=norm_groups)\n",
    "\n",
    "        # Define the second convolutional block\n",
    "        conv_2 = MyronenkoConvolutionBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                             padding=padding, norm_layer=norm_layer, norm_groups=norm_groups)\n",
    "        \n",
    "        # If the number of in channels is not equal to the number of our channels, we do 1x1x1 convolution\n",
    "        if in_channels != out_channels:\n",
    "            self.sample = conv1x1x1(in_channels, out_channels)\n",
    "        else:\n",
    "            self.sample = None\n",
    "        \n",
    "        return nn.Sequential(conv_1, conv_2)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        # Define the identity of x_input, like residual block\n",
    "        identity = x_input\n",
    "\n",
    "        # Pass the input through the network\n",
    "        for layer in self.myronenko_block:\n",
    "            x_input = layer(x_input)\n",
    "            print(\"Shape of input in MyronenkoResBlock: \", x_input.shape)\n",
    "\n",
    "        # If the sample is not None, then we do 1x1x1 convolution\n",
    "        if self.sample is not None:\n",
    "            identity = self.sample(identity)\n",
    "            print(\"Identity in res block shape: \", identity.shape)\n",
    "\n",
    "        # Add the identity to the output\n",
    "        x_input += identity\n",
    "\n",
    "        # Return the output\n",
    "        return x_input\n",
    "\n",
    "###############################################################\n",
    "####################### Myronenko Layer #######################\n",
    "###############################################################\n",
    "class MyronenkoLayer(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, num_blocks, block, in_channels, out_channels, *args, dropout=None, kernel_size=3, **kwargs):\n",
    "        super(MyronenkoLayer, self).__init__()\n",
    "        self.myronenko_layer = self.build_myronenko_layer(num_blocks, block, in_channels, out_channels, *args, dropout=dropout, kernel_size=kernel_size, **kwargs)\n",
    "\n",
    "    # Build the Myronenko layer\n",
    "    def build_myronenko_layer(self, num_blocks, block, in_channels, out_channels, *args, dropout=None, kernel_size=3, **kwargs):\n",
    "\n",
    "        # The layer will hold the blocks as a module list\n",
    "        layer = nn.ModuleList()\n",
    "\n",
    "        # For number of blocks\n",
    "        for i in range(num_blocks):\n",
    "\n",
    "            # Append the block\n",
    "            layer.append(block(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, *args, **kwargs))\n",
    "\n",
    "            # Set the in channels to the out channels\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # If dropout is not None, then we add dropout\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout3d(dropout, inplace=True)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        return layer\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x_input):\n",
    "            \n",
    "        # For each block in the layer\n",
    "        for index, block in enumerate(self.myronenko_layer):\n",
    "\n",
    "            # Pass the input through the block\n",
    "            x_input = block(x_input)\n",
    "            print(\"Shape of input in MyronenkoLayer: \", x_input.shape)\n",
    "\n",
    "            # If dropout is not None, then we add dropout\n",
    "            if index == 0 and self.dropout is not None:\n",
    "                x_input = self.dropout(x_input)\n",
    "\n",
    "        # Return the output\n",
    "        return x_input\n",
    "\n",
    "\n",
    "###############################################################\n",
    "####################### Myronenko Encoder #######################\n",
    "###############################################################\n",
    "class MyronenkoEncoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, in_channels, ngf=32, block_layers=None, layer=MyronenkoLayer,\n",
    "                 block=MyronenkoResidualBlock, feature_dilation=2, downsampling_stride=2, dropout=0.2, \n",
    "                 layer_widths=None, kernel_size=3):\n",
    "        super(MyronenkoEncoder, self).__init__()\n",
    "        self.myronenko_encoder = self.build_myronenko_encoder(in_channels, ngf, block_layers, layer, block, feature_dilation,\n",
    "                                                                downsampling_stride, dropout, layer_widths, kernel_size)\n",
    "        \n",
    "    # Build the encoder\n",
    "    def build_myronenko_encoder(self, in_channels, ngf, block_layers, layer, block, feature_dilation, downsampling_stride, \n",
    "                                dropout, layer_widths, kernel_size):\n",
    "        \n",
    "        # If the layer blocks are not specified, we use the default ones\n",
    "        if block_layers is None:\n",
    "            block_layers = [1, 2, 2, 4]\n",
    "\n",
    "        # Define layers and downsamples as ModuleLists\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsampling_convolutions = nn.ModuleList()\n",
    "\n",
    "        # For every block\n",
    "        for index, num_blocks in enumerate(block_layers):\n",
    "\n",
    "            # If the layer widths are not specified, we use the default ones\n",
    "            if layer_widths is None:\n",
    "                out_channels = ngf * (feature_dilation ** index)\n",
    "            else:\n",
    "                out_channels = layer_widths[index]\n",
    "\n",
    "            # If dropout is not None, we use it\n",
    "            if index == 0 and dropout is not None:\n",
    "                dropout_layer = dropout\n",
    "            else:\n",
    "                dropout_layer = None\n",
    "\n",
    "            # Add the layer to layers\n",
    "            self.layers.append(layer(num_blocks=num_blocks, block=block, in_channels=in_channels, out_channels=out_channels, \n",
    "                                     dropout=dropout_layer, kernel_size=kernel_size))\n",
    "\n",
    "            # If we're not at the last layer, we add a downsampling convolution\n",
    "            if index != len(block_layers) - 1:\n",
    "                self.downsampling_convolutions.append(conv3x3x3(in_channels=out_channels, out_channels=out_channels, \n",
    "                                                                stride=downsampling_stride, kernel_size=kernel_size))\n",
    "\n",
    "            # Print out the layer\n",
    "            print(\"Encoder Layer {}:\".format(index), in_channels, out_channels)\n",
    "\n",
    "            # Set the in width to the out width\n",
    "            in_channels = out_channels\n",
    "\n",
    "\n",
    "        # Zip the layers and downsampling convolutions together\n",
    "        encoder = zip(self.layers[:-1], self.downsampling_convolutions)\n",
    "\n",
    "        # Return the encoder\n",
    "        return encoder\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        # For each layer in the encoder\n",
    "        for layer, downsampling_convolution in self.myronenko_encoder:\n",
    "                \n",
    "            # Pass the input through the layer\n",
    "            x_input = layer(x_input)\n",
    "            print(\"Shape of input in MyronenkoEncoder: \", x_input.shape)\n",
    "\n",
    "            # If the downsampling convolution is not None, then we do 1x1x1 convolution\n",
    "            if downsampling_convolution is not None:\n",
    "                x_input = downsampling_convolution(x_input)\n",
    "                print(\"Shape of input in MyronenkoEncoder conv: \", x_input.shape)\n",
    "        \n",
    "        x_input = self.layers[-1](x_input)\n",
    "        print(\"Shape of input in MyronenkoEncoder: \", x_input.shape)\n",
    "\n",
    "        # Return the output\n",
    "        return x_input\n",
    "    \n",
    "###############################################################\n",
    "######################## U-Net Encoder ########################\n",
    "###############################################################\n",
    "class UNetEncoder(MyronenkoEncoder):\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        # Define the outputs\n",
    "        outputs = []\n",
    "\n",
    "        # For each layer in the encoder\n",
    "        for layer, downsampling_convolution in self.myronenko_encoder:\n",
    "            \n",
    "            # Pass the input through the layer\n",
    "            x_input = layer(x_input)\n",
    "            print(\"Shape of input in UNetEncoder: \", x_input.shape)\n",
    "\n",
    "            # Insert the output into the outputs\n",
    "            outputs.insert(0, x_input)\n",
    "\n",
    "            # If the downsampling convolution is not None, then we do 1x1x1 convolution\n",
    "            if downsampling_convolution is not None:\n",
    "                x_input = downsampling_convolution(x_input)\n",
    "                print(\"Shape of input in downsampling UNetEncoder: \", x_input.shape)\n",
    "\n",
    "        # Add the last layer to the outputs\n",
    "        x_input = self.layers[-1](x_input)\n",
    "        outputs.insert(0, x_input)\n",
    "\n",
    "        # Return the outputs\n",
    "        return outputs\n",
    "\n",
    "###############################################################\n",
    "####################### Mirrored Decoder #######################\n",
    "###############################################################\n",
    "class MirroredDecoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, ngf=32, block_layers=None, layer=MyronenkoLayer, block=MyronenkoResidualBlock,\n",
    "                upsampling_scale=2, feature_reduction_scale=2, upsampling_mode=\"trilinear\", align_corners=False,\n",
    "                layer_widths=None, use_transposed_convolutions=False, kernel_size=3):\n",
    "        super(MirroredDecoder, self).__init__()\n",
    "        self.mirrored_decoder = self.build_mirrored_decoder(ngf, block_layers, layer, block, upsampling_scale,\n",
    "                                                            feature_reduction_scale, upsampling_mode, align_corners,\n",
    "                                                            layer_widths, use_transposed_convolutions, kernel_size)\n",
    "        \n",
    "    # Build the decoder\n",
    "    def build_mirrored_decoder(self, ngf, block_layers, layer, block, upsampling_scale, feature_reduction_scale,\n",
    "                               upsampling_mode, align_corners, layer_widths, use_transposed_convolutions, kernel_size):\n",
    "        \n",
    "        # If the layer blocks are not specified, we use the default ones\n",
    "        if block_layers is None:\n",
    "            self.block_layers = [1, 1, 1, 1]\n",
    "        else:\n",
    "            self.block_layers = block_layers\n",
    "\n",
    "        # Define the widths and feature scales\n",
    "        self.ngf = ngf\n",
    "        self.feature_reduction_scale = feature_reduction_scale\n",
    "        self.layer_widths = layer_widths\n",
    "\n",
    "        # Define whether or not to use transposed convolutions\n",
    "        self.use_transposed_convolutions = use_transposed_convolutions\n",
    "\n",
    "        # Define layers and upsamples as ModuleLists\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.pre_upsampling_convolutions = nn.ModuleList()\n",
    "\n",
    "        # If we use transposed convolutions, define it as a ModuleList\n",
    "        if use_transposed_convolutions:\n",
    "            self.upsampling_convolutions = nn.ModuleList()\n",
    "        else:\n",
    "            self.upsampling_convolutions = []\n",
    "\n",
    "        # For every block\n",
    "        for index, num_blocks in enumerate(self.block_layers):\n",
    "\n",
    "            # Get the depth of the layer\n",
    "            depth = len(self.block_layers) - index - 1\n",
    "\n",
    "            # Calculate the in and out width\n",
    "            in_channels, out_channels = self.calculate_layer_widths(depth)\n",
    "\n",
    "            # If the depth isnt 0\n",
    "            if depth != 0:\n",
    "\n",
    "                # Append the layer to layers\n",
    "                self.layers.append(layer(num_blocks=num_blocks, block=block, in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size))\n",
    "\n",
    "                # If we use transposed convolutions\n",
    "                if self.use_transposed_convolutions:\n",
    "\n",
    "                    # Append conv1x1x1 to pre upsampling blocks\n",
    "                    self.pre_upsampling_convolutions.append(nn.Sequential())\n",
    "\n",
    "                    # Append the upsampling convolution\n",
    "                    self.upsampling_convolutions.append(nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                                                                     stride=upsampling_scale, padding=1))\n",
    "                else:\n",
    "                    # Append conv1x1x1 to pre upsampling blocks\n",
    "                    self.pre_upsampling_convolutions.append(conv1x1x1(in_channels=in_channels, out_channels=out_channels, stride=1))\n",
    "\n",
    "                    # Append the upsampling convolution\n",
    "                    self.upsampling_convolutions.append(partial(nn.functional.interpolate, scale_factor=upsampling_scale,\n",
    "                                                            mode=upsampling_mode, align_corners=align_corners))\n",
    "            # If the depth is 0\n",
    "            else:\n",
    "                # Add layer\n",
    "                self.layers.append(layer(num_blocks=num_blocks, block=block, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size))\n",
    "\n",
    "                # Print out the layer\n",
    "                print(\"Decoder Layer {}:\".format(index), in_channels, out_channels)\n",
    "\n",
    "        # Zip the layers and upsampling convolutions together\n",
    "        decoder = zip(self.pre_upsampling_convolutions, self.upsampling_convolutions, self.layers[:-1])\n",
    "\n",
    "        # Return the decoder\n",
    "        return decoder\n",
    "    \n",
    "    # Calculate the layer widths\n",
    "    def calculate_layer_widths(self, depth):\n",
    "        \n",
    "        # If the layer widths are specified\n",
    "        if self.layer_widths is not None:\n",
    "\n",
    "            # Get the in and out width\n",
    "            in_channels = self.layer_widths[depth + 1]\n",
    "            out_channels = self.layer_widths[depth]\n",
    "\n",
    "        # If the layer widths are not specified, we use the default ones\n",
    "        else:\n",
    "            if depth > 0:\n",
    "                out_channels = int(self.ngf * (self.feature_reduction_scale ** (depth - 1)))\n",
    "                in_channels = out_channels * self.feature_reduction_scale\n",
    "            else:\n",
    "                out_channels = self.ngf\n",
    "                in_channels = self.ngf\n",
    "\n",
    "        # Return the in and out width\n",
    "        return in_channels, out_channels\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        # For each pre upsampling convolution, upsampling convolution, and layer\n",
    "        for pre_upsampling_convolution, upsampling_convolution, layer in self.mirrored_decoder:\n",
    "\n",
    "            # Pass the input through the layer\n",
    "            x_input = layer(x_input)\n",
    "            print(\"Shape of input in MirroredDecoder: \", x_input.shape)\n",
    "\n",
    "            # Pass the input through the pre upsampling convolution\n",
    "            x_input = pre_upsampling_convolution(x_input)\n",
    "            print(\"Shape of preupsampling in MirroredDecoder: \", x_input.shape)\n",
    "\n",
    "            # Pass the input through the upsampling convolution\n",
    "            x_input = upsampling_convolution(x_input)\n",
    "            print(\"Shape of upsampling in MirroredDecoder: \", x_input.shape)\n",
    "\n",
    "        x_input = self.layers[-1](x_input)\n",
    "        print(\"Shape of concatenated in MirroredDecoder: \", x_input.shape)\n",
    "\n",
    "        # Return the output\n",
    "        return x_input\n",
    "\n",
    "###############################################################\n",
    "######################## U-Net Decoder ########################\n",
    "###############################################################\n",
    "class UNetDecoder(MirroredDecoder):\n",
    "\n",
    "    # Calculate the layer widths\n",
    "    def calculate_layer_widths(self, depth):\n",
    "\n",
    "        # Get them from MirroredDecoder\n",
    "        in_channels, out_channels = super().calculate_layer_widths(depth=depth)\n",
    "\n",
    "        # Id the deoth is not at the last block\n",
    "        if depth != len(self.block_layers) - 1:\n",
    "\n",
    "            # Double the in width\n",
    "            in_channels *= 2\n",
    "\n",
    "        # Print out the layer\n",
    "        print(\"Decoder Layer {}:\".format(depth), in_channels, out_channels)\n",
    "\n",
    "        # Return the in and out width\n",
    "        return in_channels, out_channels\n",
    "    \n",
    "    # Define the forward pass\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        # x is the first input\n",
    "        x = x_input[0]\n",
    "\n",
    "        # For each pre upsampling convolution, upsampling convolution, and layer\n",
    "        for index, (pre_upsampling_convolution, upsampling_convolution, layer) in enumerate(self.mirrored_decoder):\n",
    "\n",
    "            # Pass the input through the layer\n",
    "            x = layer(x)\n",
    "            print(\"Decoder input shape at idx {} is: {}\".format(index, x.shape))\n",
    "\n",
    "            # Pass the input through the pre upsampling convolution\n",
    "            x = pre_upsampling_convolution(x)\n",
    "            print(\"Shape of preupsampling in UnetDecoder: \", x.shape)\n",
    "\n",
    "            # Pass the input through the upsampling convolution\n",
    "            x = upsampling_convolution(x)\n",
    "            print(\"Shape of upsampling in UnetDecoder: \", x.shape)\n",
    "\n",
    "            # Concatenate\n",
    "            x = torch.cat((x, x_input[index + 1]), dim=1)\n",
    "            print(\"Shape of concatenate in UnetDecoder: \", x.shape)\n",
    "\n",
    "        # Pass the input through the last layer\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        # Return the output\n",
    "        return x\n",
    "\n",
    "###############################################################\n",
    "################## Convolutional Autoencoder ##################\n",
    "###############################################################\n",
    "class ConvolutionalAutoEncoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, input_shape=None, in_channels=3, ngf=64, encoder_blocks=None, decoder_blocks=None,\n",
    "                feature_dilation=2, downsampling_stride=2, interpolation_mode=\"trilinear\",\n",
    "                encoder_class=MyronenkoEncoder, decoder_class=None, output_channels=None, layer_widths=None,\n",
    "                decoder_mirrors_encoder=False, activation=None, use_transposed_convolutions=False, kernel_size=3,\n",
    "                voxel_wise=False):\n",
    "        super(ConvolutionalAutoEncoder, self).__init__()\n",
    "        self.convolutional_autoencoder = self.build_convolutional_autoencoder(input_shape, in_channels, ngf, encoder_blocks, decoder_blocks,\n",
    "                                                                                feature_dilation, downsampling_stride, interpolation_mode,\n",
    "                                                                                encoder_class, decoder_class, output_channels, layer_widths,\n",
    "                                                                                decoder_mirrors_encoder, activation, use_transposed_convolutions, \n",
    "                                                                                kernel_size, voxel_wise)\n",
    "\n",
    "    # Build the convolutional autoencoder\n",
    "    def build_convolutional_autoencoder(self, input_shape, in_channels, ngf, encoder_blocks, decoder_blocks,\n",
    "                                        feature_dilation, downsampling_stride, interpolation_mode, encoder_class,\n",
    "                                        decoder_class, output_channels, layer_widths, decoder_mirrors_encoder, activation,\n",
    "                                        use_transposed_convolutions, kernel_size, voxel_wise):\n",
    "        \n",
    "        # If the encoder blocks are not specified, we use the default ones\n",
    "        if encoder_blocks is None:\n",
    "            encoder_blocks = [1, 2, 2, 4]\n",
    "\n",
    "        # Define the attributes of the model\n",
    "        self.in_channels = in_channels\n",
    "        self.ngf = ngf\n",
    "        self.out_channels = output_channels\n",
    "        self.number_downsampling = 2\n",
    "        self.norm_layer = nn.BatchNorm3d\n",
    "\n",
    "        # Define whether it's voxel_wise or not\n",
    "        self.voxel_wise = voxel_wise\n",
    "\n",
    "        # Define the image and non-image models\n",
    "        self.non_img_model = self.define_non_img_model()\n",
    "        self.joint_model = self.define_joint_model()\n",
    "\n",
    "        # Define the encoder\n",
    "        self.encoder = encoder_class(in_channels=in_channels, ngf=ngf, block_layers=encoder_blocks,\n",
    "                                    feature_dilation=feature_dilation, downsampling_stride=downsampling_stride,\n",
    "                                    layer_widths=layer_widths, kernel_size=kernel_size)\n",
    "        \n",
    "        # Get the decoder class and blocks\n",
    "        decoder_class, decoder_blocks = self.set_decoder_blocks(decoder_class, encoder_blocks, decoder_mirrors_encoder,\n",
    "                                                                decoder_blocks)\n",
    "\n",
    "        # Define the decoder\n",
    "        self.decoder = decoder_class(ngf=ngf, block_layers=decoder_blocks,\n",
    "                                     upsampling_scale=downsampling_stride, feature_reduction_scale=feature_dilation,\n",
    "                                     upsampling_mode=interpolation_mode, layer_widths=layer_widths,\n",
    "                                     use_transposed_convolutions=use_transposed_convolutions,\n",
    "                                     kernel_size=kernel_size)\n",
    "        \n",
    "        # Set the final convolution\n",
    "        self.set_final_convolution(output_channels=output_channels)\n",
    "\n",
    "        # Set the activation\n",
    "        self.set_activation(activation=activation)\n",
    "\n",
    "        # Return the convolutional autoencoder\n",
    "        if self.activation is None:\n",
    "            return nn.Sequential(self.encoder, self.decoder, self.final_convolution)\n",
    "        else:\n",
    "            return nn.Sequential(self.encoder, self.decoder, self.final_convolution, self.activation)\n",
    "\n",
    "    # Set the decoder blocks\n",
    "    def set_decoder_blocks(self, decoder_class, encoder_blocks, decoder_mirrors_encoder, decoder_blocks):\n",
    "\n",
    "        # If the decoder is mirror encoder\n",
    "        if decoder_mirrors_encoder:\n",
    "            # The decoder block is the encoder block\n",
    "            decoder_blocks = encoder_blocks\n",
    "\n",
    "            # If the decoder class is not specified, we use the MirroredDecoder\n",
    "            if decoder_class is None:\n",
    "                decoder_class = MirroredDecoder\n",
    "            \n",
    "        # If the deocder blocks is None\n",
    "        elif decoder_blocks is None:\n",
    "\n",
    "            # Define it as 1 for every encoder block\n",
    "            decoder_blocks = [1] * len(encoder_blocks)\n",
    "\n",
    "            # If the decoder class is not specified, we use the MyronenkoDecoder\n",
    "            if decoder_class is None:\n",
    "                decoder_class = MyronenkoDecoder\n",
    "\n",
    "        # Return the decoder class and blocks\n",
    "        return decoder_class, decoder_blocks\n",
    "    \n",
    "     # Define the processing for the non-image inputs\n",
    "    def define_non_img_model(self):\n",
    "        \n",
    "        # Stores the model\n",
    "        model = []\n",
    "        \n",
    "        # Add convolutions for the injection centers and image coordinates - expected to have self.output_nc channels\n",
    "        for i in range(self.number_downsampling):\n",
    "            model += [nn.Conv3d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                      self.norm_layer(self.out_channels), \n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        # Return the model\n",
    "        return nn.Sequential(*model)\n",
    "            \n",
    "    # Define joint processing for everything\n",
    "    def define_joint_model(self):\n",
    "        \n",
    "        # Stores the model\n",
    "        model = []\n",
    "        \n",
    "        # Define the factor we multiply by, based on voxel_wise\n",
    "        if self.voxel_wise:\n",
    "            factor = 1\n",
    "        else:\n",
    "            factor = 3\n",
    "        \n",
    "        # Add final convolutions for image and non-image data\n",
    "        # Cube output: self.out_channels * 3 channels | Voxel output: self.out_channels channels\n",
    "        for i in range(self.number_downsampling):\n",
    "            model += [nn.Conv3d(self.out_channels * factor, self.out_channels * factor, kernel_size=3, stride=1, padding=1, \n",
    "                                bias=False),\n",
    "                      self.norm_layer(self.out_channels * factor), \n",
    "                          nn.ReLU(True)]\n",
    "            \n",
    "        # Final convolution to make the number of channels 1\n",
    "        # Cube output: self.out_channels * 3 channels | Voxel output: self.out_channels channels\n",
    "        model += [nn.Conv3d(self.out_channels * factor, 1, kernel_size=3, stride=1, padding=1, bias=False)]\n",
    "        \n",
    "        # Cube output: No Adaptive layer | Voxel output: Adaptive layer\n",
    "        if self.voxel_wise:\n",
    "            model += [nn.AdaptiveAvgPool3d((1, 1, 1))]\n",
    "            \n",
    "        # Return the model\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "    # Set the final convolution\n",
    "    def set_final_convolution(self, output_channels):\n",
    "\n",
    "        # Depending on if it's voxel_wise or not, the final convolution will either just\n",
    "        # 1. Make # channels to 1\n",
    "        # 2. Make # channels AND spatial size to 1\n",
    "\n",
    "        # Cube output: No Adaptive layer | Voxel output: Adaptive layer\n",
    "\n",
    "        # If it's voxel_wise\n",
    "        if self.voxel_wise:\n",
    "            self.final_convolution = nn.Sequential(\n",
    "                conv1x1x1(in_channels=self.ngf, out_channels=output_channels, stride=2),\n",
    "                nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "            )\n",
    "        else:\n",
    "            self.final_convolution = conv1x1x1(in_channels=self.ngf, out_channels=output_channels, stride=2)\n",
    "\n",
    "    # Set the activation\n",
    "    def set_activation(self, activation=None):\n",
    "        # If it's sigmoid\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        # If it's softmax\n",
    "        elif activation == \"softmax\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        # If it's relu\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        # If it's leaky relu\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        # If it's none\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, input_x, injection_center, image_coordinates):\n",
    "\n",
    "        # Define the dimension we concatenate along, depending on voxel wise\n",
    "        if self.voxel_wise:\n",
    "            dim = 4\n",
    "        else:\n",
    "            dim = 1\n",
    "\n",
    "        # Do all the convolutions for the b0 first\n",
    "        input_x = self.convolutional_autoencoder(input_x)\n",
    "        print(\"Shape of x after CAE: \", input_x.shape)\n",
    "\n",
    "        # Do the convolutional layers for the injection center\n",
    "        injection_center = self.non_img_model(injection_center)\n",
    "        print(\"Shape of injection center after non-img model: \", injection_center.shape)\n",
    "        \n",
    "        # Do the convolutional layers for the image coordinates\n",
    "        image_coordinates = self.non_img_model(image_coordinates)\n",
    "        print(\"Shape of image coordinates after non-img model: \", image_coordinates.shape)\n",
    "        \n",
    "        # Concatenate the data along the number of channels\n",
    "        # Cube output: Dimension 1 | Voxel output: Dimension 4\n",
    "        input_x = torch.cat((input_x, injection_center), dim=dim)\n",
    "        input_x = torch.cat((input_x, image_coordinates), dim=dim)\n",
    "        \n",
    "        # Do the joint processing\n",
    "        joint_data = self.joint_model(input_x)\n",
    "\n",
    "        # Return the model\n",
    "        return joint_data\n",
    "    \n",
    "###############################################################\n",
    "######################## U-Net General ########################\n",
    "###############################################################\n",
    "class UNet(ConvolutionalAutoEncoder):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, *args, encoder_class=UNetEncoder, decoder_class=UNetDecoder, in_channels=1, out_channels=1,\n",
    "                 voxel_wise=False, **kwargs):\n",
    "        super().__init__(*args, encoder_class=encoder_class, decoder_class=decoder_class, in_channels=in_channels,\n",
    "                         output_channels=out_channels, voxel_wise=voxel_wise, **kwargs)\n",
    "        self.set_final_convolution(output_channels=out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Layer 0: 1 64\n",
      "Encoder Layer 1: 64 128\n",
      "Encoder Layer 2: 128 256\n",
      "Encoder Layer 3: 256 512\n",
      "Decoder Layer 3: 512 256\n",
      "Decoder Layer 2: 512 128\n",
      "Decoder Layer 1: 256 64\n",
      "Decoder Layer 0: 128 64\n",
      "Decoder Layer 3: 128 64\n"
     ]
    }
   ],
   "source": [
    "model = UNet(voxel_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 1, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 1, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Identity in res block shape:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in UNetEncoder:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in downsampling UNetEncoder:  torch.Size([16, 64, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Identity in res block shape:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in UNetEncoder:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of input in downsampling UNetEncoder:  torch.Size([16, 128, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Identity in res block shape:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in UNetEncoder:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of input in downsampling UNetEncoder:  torch.Size([16, 256, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Identity in res block shape:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 4, 4, 4])\n",
      "Decoder input shape at idx 0 is: torch.Size([16, 512, 4, 4, 4])\n",
      "Shape of preupsampling in UnetDecoder:  torch.Size([16, 256, 4, 4, 4])\n",
      "Shape of upsampling in UnetDecoder:  torch.Size([16, 256, 8, 8, 8])\n",
      "Shape of concatenate in UnetDecoder:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 512, 8, 8, 8])\n",
      "Decoder input shape at idx 1 is: torch.Size([16, 512, 8, 8, 8])\n",
      "Shape of preupsampling in UnetDecoder:  torch.Size([16, 128, 8, 8, 8])\n",
      "Shape of upsampling in UnetDecoder:  torch.Size([16, 128, 16, 16, 16])\n",
      "Shape of concatenate in UnetDecoder:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 256, 16, 16, 16])\n",
      "Decoder input shape at idx 2 is: torch.Size([16, 256, 16, 16, 16])\n",
      "Shape of preupsampling in UnetDecoder:  torch.Size([16, 64, 16, 16, 16])\n",
      "Shape of upsampling in UnetDecoder:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of concatenate in UnetDecoder:  torch.Size([16, 128, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 128, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoConvBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoResBlock:  torch.Size([16, 64, 32, 32, 32])\n",
      "Identity in res block shape:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of input in MyronenkoLayer:  torch.Size([16, 64, 32, 32, 32])\n",
      "Shape of x after CAE:  torch.Size([16, 1, 1, 1, 1])\n",
      "Shape of injection center after non-img model:  torch.Size([16, 1, 1, 1, 3])\n",
      "Shape of image coordinates after non-img model:  torch.Size([16, 1, 1, 1, 3])\n",
      "output shape is:  torch.Size([16, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "output = model(b0_cube, injection_center, image_coordinates)\n",
    "\n",
    "print(\"output shape is: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'BatchNorm3d' has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResnetEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_nc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNorm3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpadding_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoxel_wise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(b0_cube, injection_center, image_coordinates)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/notebooks/Marmoset_Residual_Training/models/model_builders/encoders.py:117\u001b[0m, in \u001b[0;36mResnetEncoder.__init__\u001b[0;34m(self, input_nc, output_nc, ngf, n_blocks, norm_layer, use_dropout, padding_type, voxel_wise)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngf \u001b[38;5;241m=\u001b[39m ngf\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_blocks \u001b[38;5;241m=\u001b[39m n_blocks\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_norm_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dropout \u001b[38;5;241m=\u001b[39m use_dropout\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_type \u001b[38;5;241m=\u001b[39m padding_type\n",
      "File \u001b[0;32m/notebooks/Marmoset_Residual_Training/models/model_builders/encoders.py:253\u001b[0m, in \u001b[0;36mResnetEncoder.get_norm_layer\u001b[0;34m(self, norm_layer)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_norm_layer\u001b[39m(\u001b[38;5;28mself\u001b[39m, norm_layer):\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# If the norm layer is batch norm, we return it\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m():\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mBatchNorm3d\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m norm_layer\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BatchNorm3d' has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "model = ResnetEncoder(input_nc=45, ngf=64, n_blocks=3, norm_layer=nn.BatchNorm3d, \n",
    "                      padding_type='reflect', voxel_wise=False)\n",
    "output = model(b0_cube, injection_center, image_coordinates)\n",
    "\n",
    "print(\"output shape is: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(b0_cube, injection_center, image_coordinates)\n",
    "\n",
    "print(\"output shape is: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the output\n",
    "output = output.squeeze(0).squeeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "def glob_files(PATH_NAME, file_format):\n",
    "    INPUT_FILES = []\n",
    "    for file in glob.glob(os.path.join(PATH_NAME, os.path.join(\"**\", \"*.{}\".format(file_format))), recursive=True):\n",
    "        INPUT_FILES.append(file)\n",
    "    return INPUT_FILES\n",
    "\n",
    "nii_gz_files = glob_files(\"/notebooks/model_data_w_resize\", \"nii.gz\")\n",
    "b0_images = [file for file in nii_gz_files if \"b0\" in file and \"resized\" not in file]\n",
    "print(len(b0_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 356, 256)\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "reader = sitk.ImageFileReader()\n",
    "reader.SetFileName(b0_images[0])\n",
    "image = reader.Execute()\n",
    "\n",
    "normalizeFilter = sitk.NormalizeImageFilter()\n",
    "rescaleFilter = sitk.RescaleIntensityImageFilter()\n",
    "rescaleFilter.SetOutputMaximum(255)\n",
    "rescaleFilter.SetOutputMinimum(0)\n",
    "\n",
    "image = normalizeFilter.Execute(image)\n",
    "image = rescaleFilter.Execute(image)\n",
    "\n",
    "array = sitk.GetArrayFromImage(image)\n",
    "print(array.shape)\n",
    "\n",
    "image_normalized = sitk.GetImageFromArray(array)\n",
    "\n",
    "sitk.WriteImage(image_normalized, os.path.join(os.getcwd(), \"test2.nii.gz\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 356, 230)\n",
      "(256, 356, 230)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "image = nib.load(b0_images[0])\n",
    "data = image.get_fdata()\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "normalized_vector = data / np.linalg.norm(data)\n",
    "\n",
    "print(normalized_vector.shape)\n",
    "\n",
    "final_img = nib.Nifti1Image(normalized_vector, image.affine)\n",
    "\n",
    "nib.save(final_img, os.path.join(os.getcwd(), \"nibabel.nii\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 356, 230)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"nibabel.nii\")\n",
    "\n",
    "image = nib.load(path)\n",
    "data = image.get_fdata()\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12, 8, 8, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_to_residuals = \"/notebooks/tract_residuals/predicted_residuals/epoch_1/image_0.npy\"\n",
    "image0 = np.load(path_to_residuals)\n",
    "print(image0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 178, 115)\n"
     ]
    }
   ],
   "source": [
    "def to_shape(a, shape):\n",
    "    y_, x_, z_ = shape\n",
    "    y, x, z = a.shape\n",
    "    y_pad = (y_-y)\n",
    "    x_pad = (x_-x)\n",
    "    z_pad = (z_-z)\n",
    "    return np.pad(a,((y_pad//2, y_pad//2 + y_pad%2), \n",
    "                     (x_pad//2, x_pad//2 + x_pad%2),\n",
    "                     (z_pad//2, z_pad//2 + z_pad%2)),\n",
    "                  mode = 'constant')\n",
    "\n",
    "# Create random array\n",
    "random_array = np.random.rand(128, 178, 115)\n",
    "print(random_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3]\n",
      "[128, 180, 118]\n",
      "(128, 180, 118)\n"
     ]
    }
   ],
   "source": [
    "# Define kernel size\n",
    "kernel_size = 8 * 2\n",
    "\n",
    "# Get the number of values for each axes that need to be added to fit multiple of kernel\n",
    "padding_needed = [axis % kernel_size for axis in random_array.shape]\n",
    "print(padding_needed)\n",
    "\n",
    "output_shape = []\n",
    "for i in range(random_array.ndim):\n",
    "    output_shape.append(random_array.shape[i] + padding_needed[i])\n",
    "    \n",
    "print(output_shape)\n",
    "\n",
    "# Padding the random array to the new shape\n",
    "random_reshaped = to_shape(random_array, output_shape)\n",
    "print(random_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T23:31:31.335536Z",
     "iopub.status.busy": "2023-07-25T23:31:31.334884Z",
     "iopub.status.idle": "2023-07-25T23:31:31.470032Z",
     "shell.execute_reply": "2023-07-25T23:31:31.469379Z",
     "shell.execute_reply.started": "2023-07-25T23:31:31.335505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 178, 115)\n",
      "(128, 178, 115)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "path = \"/notebooks/model_data_w_resize/dMRI_b0/A10-R01_0028-TT21/DWI_concatenated_b0_resized.nii.gz\"\n",
    "injection_center = \"/notebooks/model_data_w_resize/injection_centers/A10-R01_0028-TT21/inj_center.csv\"\n",
    "stream_path = \"/notebooks/model_data_w_resize/tckmapped_streamlines/A10-R01_0028-TT21/subtracted_unflipped_resized.nii.gz\"\n",
    "\n",
    "# Load the image\n",
    "image = nib.load(path)\n",
    "streamline = nib.load(stream_path)\n",
    "data = image.get_fdata()\n",
    "stream_data = streamline.get_fdata()\n",
    "print(data.shape)\n",
    "print(stream_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T23:33:39.051685Z",
     "iopub.status.busy": "2023-07-25T23:33:39.050902Z",
     "iopub.status.idle": "2023-07-25T23:33:39.153253Z",
     "shell.execute_reply": "2023-07-25T23:33:39.152665Z",
     "shell.execute_reply.started": "2023-07-25T23:33:39.051662Z"
    }
   },
   "outputs": [],
   "source": [
    "b0_hemi = data[64:, :, :]\n",
    "res_hemi = stream_data[64:,:,:]\n",
    "\n",
    "img = nib.Nifti1Image(b0_hemi, affine=np.eye(4))\n",
    "img2 = nib.Nifti1Image(res_hemi, affine=np.eye(4))\n",
    "\n",
    "img_b0 = nib.Nifti1Image(data, affine=np.eye(4))\n",
    "img_str = nib.Nifti1Image(stream_data, affine=np.eye(4))\n",
    "\n",
    "nib.save(img, \"testingcut.nii\")\n",
    "nib.save(img2, \"testingcut_res.nii\")\n",
    "nib.save(img_b0, \"ogb0.nii\")\n",
    "nib.save(img_str, \"ogres.nii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
