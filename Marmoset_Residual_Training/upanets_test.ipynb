{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f0b43f-f074-4ca3-b92a-458bacb48ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2942c78f-7406-4962-a637-97d0be4d4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "# Define the CPA (Channel Pixel Attention) block\n",
    "class CPA(nn.Module):\n",
    "    \"\"\"\n",
    "    *same=False:\n",
    "    This scenario can be easily embedded after any CNNs, if size is same.\n",
    "    x (OG) ---------------\n",
    "    |                    |\n",
    "    sc_x (from CNNs)     CPA(x)\n",
    "    |                    |\n",
    "    out + <---------------\n",
    "    \n",
    "    *same=True:\n",
    "    This can be embedded after the CNNs where the size are different.\n",
    "    x (OG) ---------------\n",
    "    |                    |\n",
    "    sc_x (from CNNs)     |\n",
    "    |                    CPA(x)\n",
    "    CPA(sc_x)            |\n",
    "    |                    |\n",
    "    out + <---------------\n",
    "        \n",
    "    *sc_x=False\n",
    "    This operation can be seen a channel embedding with CPA\n",
    "    EX: x (3, 32, 32) => (16, 32, 32)\n",
    "    x (OG) \n",
    "    |      \n",
    "    CPA(x)\n",
    "    |    \n",
    "    out \n",
    "    \"\"\"\n",
    "     \n",
    "    # Constructor\n",
    "    def __init__(self, in_dimension, out_dimension, stride=1, same=False, sc_x=True):\n",
    "    \n",
    "        # Call parent constructor\n",
    "        super(CPA, self).__init__()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.stride = stride\n",
    "        self.same = same\n",
    "        self.sc_x = sc_x\n",
    "\n",
    "        # Define the CP_FFC layer\n",
    "        self.CP_FFC = nn.Linear(in_dimension, out_dimension)\n",
    "        self.BN = nn.BatchNorm3d(out_dimension)\n",
    "\n",
    "        # If the stride is 2 or they're the same\n",
    "        if stride == 2 or same:\n",
    "            # If sc_x is true\n",
    "            if sc_x:\n",
    "                # Define the CP_FFC layer\n",
    "                self.CP_FFC_sc = nn.Linear(in_dimension, out_dimension)\n",
    "                self.BN_sc = nn.BatchNorm3d(out_dimension)\n",
    "\n",
    "            # If it's just that the stride is 2\n",
    "            if stride == 2:\n",
    "                # Define the average pooling layer\n",
    "                self.avg_pool = nn.AvgPool3d((2, 2, 2))\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x, sc_x):\n",
    "     \n",
    "        # Get the shape of the input\n",
    "        b, c, h, w, d = x.shape\n",
    "\n",
    "        # Rearrange the input\n",
    "        out = rearrange(x, 'b c h w d -> b h w d c', c=c, h=h, w=w, d=d)\n",
    "        out = self.CP_FFC(out)\n",
    "        out = rearrange(out, 'b h w d c -> b c h w d', c=self.out_dimension, h=h, w=w, d=d)\n",
    "        out = self.BN(out)\n",
    "\n",
    "        # If they have the same shape\n",
    "        if out.shape == sc_x.shape:\n",
    "            # If sc_x is true\n",
    "            if self.sc_x:\n",
    "                # Add the two\n",
    "                out = out + sc_x\n",
    "            # Layer norm\n",
    "            out = F.layer_norm(out, out.size()[1:])\n",
    "        \n",
    "        # If they're not the same shape\n",
    "        else:\n",
    "            # Layer norm\n",
    "            out = F.layer_norm(out, out.size()[1:])\n",
    "            # If sc_x is true\n",
    "            if self.sc_x:\n",
    "                # Set x to sc_x\n",
    "                x = sc_x\n",
    "        \n",
    "        # If the stride is 2\n",
    "        if self.stride == 2 or self.same:\n",
    "            # If sc_x is true\n",
    "            if self.sc_x:\n",
    "                # Get the shape of the input\n",
    "                _, c, h, w, d = x.shape\n",
    "                # Rearrange the input\n",
    "                x = rearrange(x, 'b c h w d -> b (h w d) c')\n",
    "                x = self.CP_FFC_sc(x)\n",
    "                x = rearrange(x, 'b (h w d) c -> b c h w d', h=h, w=w, d=d)\n",
    "                x = self.BN_sc(x)\n",
    "                out = out + x   \n",
    "\n",
    "            # If they're the same\n",
    "            if self.same:\n",
    "                # Return out\n",
    "                return out\n",
    "\n",
    "            # Average pool\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        # Return out\n",
    "        return out  \n",
    "\n",
    "# Define the spatial pixel attention\n",
    "class SPA(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, img, out=1):\n",
    "\n",
    "        # Call parent constructor\n",
    "        super(SPA, self).__init__()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.SP_FFC = nn.Sequential(\n",
    "            nn.Linear(img**3, out**3),\n",
    "        )\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Get the shape of the input\n",
    "        b, c, h, w, d = x.shape\n",
    "\n",
    "        # Rearrange the input\n",
    "        x = rearrange(x, 'b c h w d -> b c (h w d)', c=c, w=w, h=h, d=d)\n",
    "        x = self.SP_FFC(x)\n",
    "        # Get the shape of x\n",
    "        print(\"x.shape\", x.shape)\n",
    "        _, c, l = x.shape\n",
    "        # Rearrange the input\n",
    "        out = rearrange(x, 'b c (h w d) -> b c h w d', c=c, w=int(l**0.5), h=int(l**0.5), d=int(l**0.5))\n",
    "\n",
    "        # Return out\n",
    "        return out\n",
    "\n",
    "# UPA (Universal Pixel Attention) block\n",
    "class UPA_Block(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, in_channels, out_channels, stride=1, cat=False, same=False, w=2, l=2):\n",
    "\n",
    "        # Call parent constructor\n",
    "        super(UPA_Block, self).__init__()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.cat = cat\n",
    "        self.same = same\n",
    "\n",
    "        # Create the first convolutional layer\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, int(out_channels * w), kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(int(out_channels * w)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(int(out_channels * w), out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Define the CNN depending on the number of layers\n",
    "        if l == 1:\n",
    "            w = 1\n",
    "            self.CNN = nn.Sequential(\n",
    "                 nn.Conv3d(in_channels, int(out_channels * w), kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm3d(int(out_channels * w)),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        # Define the attention layer\n",
    "        self.attention = CPA(in_channels, out_channels, stride, same=same)\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply the CNN\n",
    "        out = self.CNN(x)\n",
    "\n",
    "        # Apply the attention\n",
    "        out = self.attention(x, out)\n",
    "\n",
    "        # If cat is true\n",
    "        if self.cat:\n",
    "            # Concatenate\n",
    "            out = torch.cat([x, out], 1)\n",
    "\n",
    "        # Return out\n",
    "        return out\n",
    "    \n",
    "# Define the UPA Net\n",
    "class upanets(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, block, num_blocks, filter_nums, output_nc=3, img=32):\n",
    "\n",
    "        # Call parent constructor\n",
    "        super(upanets, self).__init__()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.in_channels = filter_nums\n",
    "        self.filters = filter_nums\n",
    "        w = 2\n",
    "\n",
    "        # Define the first convolutional layer\n",
    "        self.root = nn.Sequential(\n",
    "            nn.Conv3d(3, int(self.in_channels * w), kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(int(self.in_channels * w)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(int(self.in_channels * w), self.in_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(self.in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Define the first embedding\n",
    "        self.embedding = CPA(3, self.in_channels, same=True)\n",
    "\n",
    "        # Define the layers\n",
    "        self.layer1 = self._make_layer(block, int(self.filters * 1), num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(block, int(self.filters * 2), num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(block, int(self.filters * 4), num_blocks[2], 2)\n",
    "        self.layer4 = self._make_layer(block, int(self.filters * 8), num_blocks[3], 2)\n",
    "\n",
    "        # Define the SPA layers\n",
    "        self.SPA0 = SPA(img)\n",
    "        self.SPA1 = SPA(img)\n",
    "        self.SPA2 = SPA(int(img * 0.5))\n",
    "        self.SPA3 = SPA(int(img * 0.25))\n",
    "        self.SPA4 = SPA(int(img * 0.125))\n",
    "\n",
    "        # Define the linear layer\n",
    "        self.linear = nn.Linear(int(self.filters * 31), output_nc)\n",
    "\n",
    "        # Define the batchnorm\n",
    "        self.BN = nn.BatchNorm1d(int(self.filters * 31))\n",
    "\n",
    "    # Make layer function\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "\n",
    "        # Initialize parameters\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        self.out_channels = out_channels\n",
    "        out_channels = out_channels // num_blocks\n",
    "\n",
    "        # For every stride\n",
    "        for i, stride in enumerate(strides):\n",
    "\n",
    "            # If first stride and stride is 1\n",
    "            if i == 0 and stride == 1:\n",
    "                layers.append(block(self.out_channels, self.out_channels, stride, same=True))\n",
    "                strides.append(1)\n",
    "                self.in_channels = self.out_channels\n",
    "                \n",
    "            elif i != 0 and stride == 1:\n",
    "                layers.append(block(self.in_channels, out_channels, stride, cat=True))                \n",
    "                self.in_channels = self.in_channels + out_channels \n",
    "                    \n",
    "            else:   \n",
    "                layers.append(block(self.in_channels, self.out_channels, stride))\n",
    "                strides.append(1)\n",
    "                self.in_channels = self.out_channels\n",
    "                \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "\n",
    "        out01 = self.root(x)\n",
    "        print(\"out01 shape: \", out01.shape)\n",
    "        out0 = self.embedding(x, out01)\n",
    "        print(\"out0 shape: \", out0.shape)\n",
    "        \n",
    "        out1 = self.layer1(out0)\n",
    "        print(\"out1 shape: \", out1.shape)\n",
    "        out2 = self.layer2(out1)\n",
    "        print(\"out2 shape: \", out2.shape)\n",
    "        out3 = self.layer3(out2)\n",
    "        print(\"out3 shape: \", out3.shape)\n",
    "        out4 = self.layer4(out3)\n",
    "        print(\"out4 shape: \", out4.shape)\n",
    "\n",
    "        out0_spa = self.SPA0(out0)\n",
    "        print(\"out0_spa shape: \", out0_spa.shape)\n",
    "        out1_spa = self.SPA1(out1)\n",
    "        print(\"out1_spa shape: \", out1_spa.shape)\n",
    "        out2_spa = self.SPA2(out2)\n",
    "        print(\"out2_spa shape: \", out2_spa.shape)\n",
    "        out3_spa = self.SPA3(out3)\n",
    "        print(\"out3_spa shape: \", out3_spa.shape)\n",
    "        out4_spa = self.SPA4(out4)\n",
    "        print(\"out4_spa shape: \", out4_spa.shape)\n",
    "        \n",
    "        out0_gap = F.avg_pool3d(out0, out0.size()[2:])\n",
    "        print(\"out0_gap shape: \", out0_gap.shape)\n",
    "        out1_gap = F.avg_pool3d(out1, out1.size()[2:])\n",
    "        print(\"out1_gap shape: \", out1_gap.shape)\n",
    "        out2_gap = F.avg_pool3d(out2, out2.size()[2:])\n",
    "        print(\"out2_gap shape: \", out2_gap.shape)\n",
    "        out3_gap = F.avg_pool3d(out3, out3.size()[2:])\n",
    "        print(\"out3_gap shape: \", out3_gap.shape)\n",
    "        out4_gap = F.avg_pool3d(out4, out4.size()[2:])\n",
    "        print(\"out4_gap shape: \", out4_gap.shape)\n",
    "      \n",
    "        out0 = out0_gap + out0_spa\n",
    "        print(\"out0 shape: \", out0.shape)\n",
    "        out1 = out1_gap + out1_spa\n",
    "        print(\"out1 shape: \", out1.shape)\n",
    "        out2 = out2_gap + out2_spa\n",
    "        print(\"out2 shape: \", out2.shape)\n",
    "        out3 = out3_gap + out3_spa\n",
    "        print(\"out3 shape: \", out3.shape)\n",
    "        out4 = out4_gap + out4_spa\n",
    "        print(\"out4 shape: \", out4.shape)\n",
    "        \n",
    "        out0 = F.layer_norm(out0, out0.size()[1:])\n",
    "        print(\"out0 shape: \", out0.shape)\n",
    "        out1 = F.layer_norm(out1, out1.size()[1:])\n",
    "        print(\"out1 shape: \", out1.shape)\n",
    "        out2 = F.layer_norm(out2, out2.size()[1:])\n",
    "        print(\"out2 shape: \", out2.shape)\n",
    "        out3 = F.layer_norm(out3, out3.size()[1:])\n",
    "        print(\"out3 shape: \", out3.shape)\n",
    "        out4 = F.layer_norm(out4, out4.size()[1:])\n",
    "        print(\"out4 shape: \", out4.shape)\n",
    "        \n",
    "        out = torch.cat([out4, out3, out2, out1, out0], 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = self.BN(out) # please exclude when using the test function\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out        \n",
    "    \n",
    "# Define the actual UPA nets\n",
    "def UPANets(input_nc, output_nc=3, num_blocks=1, img_size=32):\n",
    "    \n",
    "    # Return the architecture\n",
    "    return upanets(block=UPA_Block,\n",
    "                   num_blocks=[int(4*num_blocks), int(4*num_blocks), int(4*num_blocks), int(4*num_blocks)],\n",
    "                   filter_nums=input_nc,\n",
    "                   output_nc=output_nc,\n",
    "                   img=img_size)\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2a1ddf2-bf9e-4865-88ab-cc8c0d7e8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out01 shape:  torch.Size([1, 16, 64, 64, 64])\n",
      "out0 shape:  torch.Size([1, 16, 64, 64, 64])\n",
      "out1 shape:  torch.Size([1, 32, 64, 64, 64])\n",
      "out2 shape:  torch.Size([1, 64, 32, 32, 32])\n",
      "out3 shape:  torch.Size([1, 128, 16, 16, 16])\n",
      "out4 shape:  torch.Size([1, 256, 8, 8, 8])\n",
      "x.shape torch.Size([1, 16, 1])\n",
      "out0_spa shape:  torch.Size([1, 16, 1, 1, 1])\n",
      "x.shape torch.Size([1, 32, 1])\n",
      "out1_spa shape:  torch.Size([1, 32, 1, 1, 1])\n",
      "x.shape torch.Size([1, 64, 1])\n",
      "out2_spa shape:  torch.Size([1, 64, 1, 1, 1])\n",
      "x.shape torch.Size([1, 128, 1])\n",
      "out3_spa shape:  torch.Size([1, 128, 1, 1, 1])\n",
      "x.shape torch.Size([1, 256, 1])\n",
      "out4_spa shape:  torch.Size([1, 256, 1, 1, 1])\n",
      "out0_gap shape:  torch.Size([1, 16, 1, 1, 1])\n",
      "out1_gap shape:  torch.Size([1, 32, 1, 1, 1])\n",
      "out2_gap shape:  torch.Size([1, 64, 1, 1, 1])\n",
      "out3_gap shape:  torch.Size([1, 128, 1, 1, 1])\n",
      "out4_gap shape:  torch.Size([1, 256, 1, 1, 1])\n",
      "out0 shape:  torch.Size([1, 16, 1, 1, 1])\n",
      "out1 shape:  torch.Size([1, 32, 1, 1, 1])\n",
      "out2 shape:  torch.Size([1, 64, 1, 1, 1])\n",
      "out3 shape:  torch.Size([1, 128, 1, 1, 1])\n",
      "out4 shape:  torch.Size([1, 256, 1, 1, 1])\n",
      "out0 shape:  torch.Size([1, 16, 1, 1, 1])\n",
      "out1 shape:  torch.Size([1, 32, 1, 1, 1])\n",
      "out2 shape:  torch.Size([1, 64, 1, 1, 1])\n",
      "out3 shape:  torch.Size([1, 128, 1, 1, 1])\n",
      "out4 shape:  torch.Size([1, 256, 1, 1, 1])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    \n",
    "    net = UPANets(16, 10, 1, 64)\n",
    "    y = net(torch.randn(1, 3, 64, 64, 64))\n",
    "    print(y.size())\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7102e13-9f35-45d3-9bd1-8efa09e508b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UPANets in PyTorch.\n",
    "by Ching-Hsun Tseng and Jia-Nan Feng\n",
    "'''\n",
    "#%%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class upa_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1, cat=False, same=False, w=2, l=2):\n",
    "        \n",
    "        super(upa_block, self).__init__()\n",
    "        \n",
    "        self.cat = cat\n",
    "        self.stride = stride\n",
    "        self.planes = planes\n",
    "        self.same = same\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, int(planes * w), kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(int(planes * w)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(int(planes * w), planes, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        if l == 1:\n",
    "            w = 1\n",
    "            self.cnn = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, int(planes * w), kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(int(planes * w)),\n",
    "                nn.ReLU(),\n",
    "                )\n",
    "        \n",
    "        self.att = CPA(in_planes, planes, stride, same=same)\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.cnn(x)\n",
    "        out = self.att(x, out)\n",
    "\n",
    "        if self.cat == True:\n",
    "            out = torch.cat([x, out], 1)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class CPA(nn.Module):\n",
    "    '''Channel Pixel Attention'''\n",
    "    \n",
    "#      *same=False:\n",
    "#       This scenario can be easily embedded after any CNNs, if size is same.\n",
    "#        x (OG) ---------------\n",
    "#        |                    |\n",
    "#        sc_x (from CNNs)     CPA(x)\n",
    "#        |                    |\n",
    "#        out + <---------------\n",
    "#        \n",
    "#      *same=True:\n",
    "#       This can be embedded after the CNNs where the size are different.\n",
    "#        x (OG) ---------------\n",
    "#        |                    |\n",
    "#        sc_x (from CNNs)     |\n",
    "#        |                    CPA(x)\n",
    "#        CPA(sc_x)            |\n",
    "#        |                    |\n",
    "#        out + <---------------\n",
    "#           \n",
    "#      *sc_x=False\n",
    "#       This operation can be seen a channel embedding with CPA\n",
    "#       EX: x (3, 32, 32) => (16, 32, 32)\n",
    "#        x (OG) \n",
    "#        |      \n",
    "#        CPA(x)\n",
    "#        |    \n",
    "#        out \n",
    "\n",
    "    def __init__(self, in_dim, dim, stride=1, same=False, sc_x=True):\n",
    "        \n",
    "        super(CPA, self).__init__()\n",
    "            \n",
    "        self.dim = dim\n",
    "        self.stride = stride\n",
    "        self.same = same\n",
    "        self.sc_x = sc_x\n",
    "        \n",
    "        self.cp_ffc = nn.Linear(in_dim, dim)\n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "\n",
    "        if self.stride == 2 or self.same == True:\n",
    "            if sc_x == True:\n",
    "                self.cp_ffc_sc = nn.Linear(in_dim, dim)\n",
    "                self.bn_sc = nn.BatchNorm2d(dim)\n",
    "            \n",
    "            if self.stride == 2:\n",
    "                self.avgpool = nn.AvgPool2d(2)\n",
    "            \n",
    "    def forward(self, x, sc_x):    \n",
    "       \n",
    "        _, c, w, h = x.shape\n",
    "        out = rearrange(x, 'b c w h -> b w h c', c=c, w=w, h=h)\n",
    "        out = self.cp_ffc(out)\n",
    "        out = rearrange(out, 'b w h c-> b c w h', c=self.dim, w=w, h=h)\n",
    "        out = self.bn(out)  \n",
    "       \n",
    "        if out.shape == sc_x.shape:\n",
    "            if self.sc_x == True:\n",
    "                out = sc_x + out\n",
    "            out = F.layer_norm(out, out.size()[1:])\n",
    "            \n",
    "        else:\n",
    "            out = F.layer_norm(out, out.size()[1:])\n",
    "            if self.sc_x == True:\n",
    "                x = sc_x\n",
    "            \n",
    "        if self.stride == 2 or self.same == True:\n",
    "            if self.sc_x == True:\n",
    "                _, c, w, h = x.shape\n",
    "                x = rearrange(x, 'b c w h -> b w h c', c=c, w=w, h=h)\n",
    "                x = self.cp_ffc_sc(x)\n",
    "                x = rearrange(x, 'b w h c-> b c w h', c=self.dim, w=w, h=h)\n",
    "                x = self.bn_sc(x)\n",
    "                out = out + x \n",
    "            \n",
    "            if self.same == True:\n",
    "                return out\n",
    "            \n",
    "            out = self.avgpool(out)\n",
    "           \n",
    "        return out\n",
    "\n",
    "   \n",
    "class SPA(nn.Module):\n",
    "    '''Spatial Pixel Attention'''\n",
    "\n",
    "    def __init__(self, img, out=1):\n",
    "        \n",
    "        super(SPA, self).__init__()\n",
    "        \n",
    "        self.sp_ffc = nn.Sequential(\n",
    "            nn.Linear(img**2, out**2)\n",
    "            )   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        _, c, w, h = x.shape          \n",
    "        x = rearrange(x, 'b c w h -> b c (w h)', c=c, w=w, h=h)\n",
    "        x = self.sp_ffc(x)\n",
    "        _, c, l = x.shape        \n",
    "        out = rearrange(x, 'b c (w h) -> b c w h', c=c, w=int(l**0.5), h=int(l**0.5))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class upanets(nn.Module):\n",
    "    def __init__(self, block, num_blocks, filter_nums, num_classes=100, img=32):\n",
    "        \n",
    "        super(upanets, self).__init__()\n",
    "        \n",
    "        self.in_planes = filter_nums\n",
    "        self.filters = filter_nums\n",
    "        w = 2\n",
    "        \n",
    "        self.root = nn.Sequential(\n",
    "                nn.Conv2d(3, int(self.in_planes*w), kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(int(self.in_planes*w)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(int(self.in_planes*w), self.in_planes*1, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(self.in_planes),\n",
    "                nn.ReLU(),\n",
    "                )        \n",
    "        self.emb = CPA(3, self.in_planes, same=True)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, int(self.filters*1), num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(block, int(self.filters*2), num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(block, int(self.filters*4), num_blocks[2], 2)\n",
    "        self.layer4 = self._make_layer(block, int(self.filters*8), num_blocks[3], 2)\n",
    "        \n",
    "        self.spa0 = SPA(img)\n",
    "        self.spa1 = SPA(img)\n",
    "        self.spa2 = SPA(int(img*0.5))\n",
    "        self.spa3 = SPA(int(img*0.25))\n",
    "        self.spa4 = SPA(int(img*0.125))\n",
    "\n",
    "        self.linear = nn.Linear(int(self.filters*31), num_classes)\n",
    "        self.bn = nn.BatchNorm1d(int(self.filters*31))\n",
    "     \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1]*(num_blocks - 1)\n",
    "        layers = []\n",
    "        self.planes = planes\n",
    "        planes = planes // num_blocks\n",
    "\n",
    "        for i, stride in enumerate(strides):\n",
    "            \n",
    "            if i == 0 and stride == 1:\n",
    "                layers.append(block(self.planes, self.planes, stride, same=True))\n",
    "                strides.append(1)\n",
    "                self.in_planes = self.planes\n",
    "                \n",
    "            elif i != 0 and stride == 1:\n",
    "                layers.append(block(self.in_planes, planes, stride, cat=True))                \n",
    "                self.in_planes = self.in_planes + planes \n",
    "                    \n",
    "            else:   \n",
    "                layers.append(block(self.in_planes, self.planes, stride))\n",
    "                strides.append(1)\n",
    "                self.in_planes = self.planes\n",
    "                \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        out01 = self.root(x)\n",
    "        print(\"out01 shape:\", out01.shape)\n",
    "        out0 = self.emb(x, out01)\n",
    "        print(\"out0 shape:\", out0.shape)\n",
    "        \n",
    "        out1 = self.layer1(out0)\n",
    "        print(\"out1 shape:\", out1.shape)\n",
    "        out2 = self.layer2(out1)\n",
    "        print(\"out2 shape:\", out2.shape)\n",
    "        out3 = self.layer3(out2)\n",
    "        print(\"out3 shape:\", out3.shape)\n",
    "        out4 = self.layer4(out3)\n",
    "        print(\"out4 shape:\", out4.shape)\n",
    "\n",
    "        out0_spa = self.spa0(out0)\n",
    "        print(\"out0_spa shape:\", out0_spa.shape)\n",
    "        out1_spa = self.spa1(out1)\n",
    "        print(\"out1_spa shape:\", out1_spa.shape)\n",
    "        out2_spa = self.spa2(out2)\n",
    "        print(\"out2_spa shape:\", out2_spa.shape)\n",
    "        out3_spa = self.spa3(out3)\n",
    "        print(\"out3_spa shape:\", out3_spa.shape)\n",
    "        out4_spa = self.spa4(out4)\n",
    "        print(\"out4_spa shape:\", out4_spa.shape)\n",
    "        \n",
    "        out0_gap = F.avg_pool2d(out0, out0.size()[2:])\n",
    "        print(\"out0_gap shape:\", out0_gap.shape)\n",
    "        out1_gap = F.avg_pool2d(out1, out1.size()[2:])\n",
    "        print(\"out1_gap shape:\", out1_gap.shape)\n",
    "        out2_gap = F.avg_pool2d(out2, out2.size()[2:])\n",
    "        print(\"out2_gap shape:\", out2_gap.shape)\n",
    "        out3_gap = F.avg_pool2d(out3, out3.size()[2:])\n",
    "        print(\"out3_gap shape:\", out3_gap.shape)\n",
    "        out4_gap = F.avg_pool2d(out4, out4.size()[2:])\n",
    "        print(\"out4_gap shape:\", out4_gap.shape)\n",
    "      \n",
    "        out0 = out0_gap + out0_spa\n",
    "        print(\"out0 shape:\", out0.shape)\n",
    "        out1 = out1_gap + out1_spa\n",
    "        print(\"out1 shape:\", out1.shape)\n",
    "        out2 = out2_gap + out2_spa\n",
    "        print(\"out2 shape:\", out2.shape)\n",
    "        out3 = out3_gap + out3_spa\n",
    "        print(\"out3 shape:\", out3.shape)\n",
    "        out4 = out4_gap + out4_spa\n",
    "        print(\"out4 shape:\", out4.shape)\n",
    "        \n",
    "        out0 = F.layer_norm(out0, out0.size()[1:])\n",
    "        print(\"out0 shape:\", out0.shape)\n",
    "        out1 = F.layer_norm(out1, out1.size()[1:])\n",
    "        print(\"out1 shape:\", out1.shape)\n",
    "        out2 = F.layer_norm(out2, out2.size()[1:])\n",
    "        print(\"out2 shape:\", out2.shape)\n",
    "        out3 = F.layer_norm(out3, out3.size()[1:])\n",
    "        print(\"out3 shape:\", out3.shape)\n",
    "        out4 = F.layer_norm(out4, out4.size()[1:])\n",
    "        print(\"out4 shape:\", out4.shape)\n",
    "        \n",
    "        out = torch.cat([out4, out3, out2, out1, out0], 1)\n",
    "        print(\"out shape:\", out.shape)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = self.bn(out) # please exclude when using the test function\n",
    "        out = self.linear(out)\n",
    "        print(\"out shape:\", out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "def UPANets(f, c = 100, block = 1, img = 32):\n",
    "    \n",
    "    return upanets(upa_block, [int(4*block), int(4*block), int(4*block), int(4*block)], f, num_classes=c, img=img)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14fa0377-c3de-41a3-8811-5844e35b3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out01 shape: torch.Size([1, 16, 64, 64])\n",
      "out0 shape: torch.Size([1, 16, 64, 64])\n",
      "out1 shape: torch.Size([1, 32, 64, 64])\n",
      "out2 shape: torch.Size([1, 64, 32, 32])\n",
      "out3 shape: torch.Size([1, 128, 16, 16])\n",
      "out4 shape: torch.Size([1, 256, 8, 8])\n",
      "out0_spa shape: torch.Size([1, 16, 1, 1])\n",
      "out1_spa shape: torch.Size([1, 32, 1, 1])\n",
      "out2_spa shape: torch.Size([1, 64, 1, 1])\n",
      "out3_spa shape: torch.Size([1, 128, 1, 1])\n",
      "out4_spa shape: torch.Size([1, 256, 1, 1])\n",
      "out0_gap shape: torch.Size([1, 16, 1, 1])\n",
      "out1_gap shape: torch.Size([1, 32, 1, 1])\n",
      "out2_gap shape: torch.Size([1, 64, 1, 1])\n",
      "out3_gap shape: torch.Size([1, 128, 1, 1])\n",
      "out4_gap shape: torch.Size([1, 256, 1, 1])\n",
      "out0 shape: torch.Size([1, 16, 1, 1])\n",
      "out1 shape: torch.Size([1, 32, 1, 1])\n",
      "out2 shape: torch.Size([1, 64, 1, 1])\n",
      "out3 shape: torch.Size([1, 128, 1, 1])\n",
      "out4 shape: torch.Size([1, 256, 1, 1])\n",
      "out0 shape: torch.Size([1, 16, 1, 1])\n",
      "out1 shape: torch.Size([1, 32, 1, 1])\n",
      "out2 shape: torch.Size([1, 64, 1, 1])\n",
      "out3 shape: torch.Size([1, 128, 1, 1])\n",
      "out4 shape: torch.Size([1, 256, 1, 1])\n",
      "out shape: torch.Size([1, 496, 1, 1])\n",
      "out shape: torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    \n",
    "    net = UPANets(16, 10, 1, 64)\n",
    "    y = net(torch.randn(1, 3, 64, 64))\n",
    "    print(y.size())\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
