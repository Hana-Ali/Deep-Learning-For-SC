{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on the HPC, need to install everything in the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the configs dictionary\n",
    "configs = {\n",
    "\n",
    "    ####### Model #######\n",
    "    \"model_name\" : \"resnet\", # Model name\n",
    "    \"input_nc\" : 1, # Number of input channels\n",
    "    \"output_nc\" : 1, # Number of output channels\n",
    "    \"ngf\" : 64, # Number of filters in first conv layer\n",
    "    \"num_blocks\" : 9, # Number of residual blocks\n",
    "    \"norm_layer\" : \"BatchNorm3d\", # Normalization layer\n",
    "    \"use_dropout\" : False, # Dropout layers\n",
    "    \"padding_type\" : \"reflect\", # Padding type\n",
    "    \n",
    "    ####### Training #######\n",
    "    \"n_epochs\" : 100, # Number of epochs\n",
    "    \"loss\" : \"mse_loss\", # Loss function\n",
    "    \"optimizer\" : \"Adam\", # Optimizer\n",
    "    \"evaluation_metric\" : \"mse_loss\", # Evaluation metric\n",
    "    \"save_best\" : True, # Save best model\n",
    "    \"regularized\" : False, # Regularization\n",
    "    \"vae\" : False, # Variational autoencoder\n",
    "\n",
    "    ####### Data #######\n",
    "    \"main_data_path\" : \"D:\\\\Brain-MINDS\\\\model_data\", # Data path\n",
    "    \"training_log_path\" : \"D:\\\\Brain-MINDS\\\\tract_residuals\\\\logs\\\\resnet_log.csv\", # Training log path\n",
    "    \"model_filename\" : \"D:\\\\Brain-MINDS\\\\tract_residuals\\\\models\\\\resnet.h5\", # Model filename\n",
    "    \"batch_size\" : 1, # Batch size\n",
    "    \"validation_batch_size\" : 1, # Validation batch size\n",
    "    \n",
    "    ####### Parameters #######\n",
    "    \"initial_learning_rate\" : 1e-04, # Initial learning rate\n",
    "    \"early_stopping_patience\": 50, # Early stopping patience\n",
    "    \"decay_patience\": 20, # Learning rate decay patience\n",
    "    \"decay_factor\": 0.5, # Learning rate decay factor\n",
    "    \"min_learning_rate\": 1e-08, # Minimum learning rate\n",
    "    \"save_last_n_models\": 10, # Save last n models\n",
    "\n",
    "    ####### Misc #######\n",
    "    \"skip_val\" : True, # Skip validation\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration path and save it as a .json file\n",
    "config_path = os.path.join(\"configs\", configs[\"model_name\"] + \".json\")\n",
    "\n",
    "# Save the configuration\n",
    "dump_json(configs, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is: ResnetEncoder\n",
      "Criterion is: <function mse_loss at 0x00000203D94205E0>\n",
      "Optimizer is: Adam\n",
      "Batch: 0\n",
      "B0: torch.Size([1, 256, 356, 230])\n",
      "Residual: torch.Size([1, 256, 356, 230])\n",
      "Injection center: torch.Size([1, 3])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 2.00 GiB total capacity; 376.33 MiB already allocated; 1.13 GiB free; 430.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\test2.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shahi/OneDrive%20-%20Imperial%20College%20London/Documents/imperial/Dissertation/Notebooks/MyCodes/Marmoset_Residual_Training/test2.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     groups \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shahi/OneDrive%20-%20Imperial%20College%20London/Documents/imperial/Dissertation/Notebooks/MyCodes/Marmoset_Residual_Training/test2.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model_metrics \u001b[39m=\u001b[39m (configs[\u001b[39m\"\u001b[39m\u001b[39mevaluation_metric\u001b[39m\u001b[39m\"\u001b[39m],)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shahi/OneDrive%20-%20Imperial%20College%20London/Documents/imperial/Dissertation/Notebooks/MyCodes/Marmoset_Residual_Training/test2.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m run_pytorch_training(configs, configs[\u001b[39m\"\u001b[39;49m\u001b[39mmodel_filename\u001b[39;49m\u001b[39m\"\u001b[39;49m], configs[\u001b[39m\"\u001b[39;49m\u001b[39mtraining_log_path\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shahi/OneDrive%20-%20Imperial%20College%20London/Documents/imperial/Dissertation/Notebooks/MyCodes/Marmoset_Residual_Training/test2.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                      metric_to_monitor\u001b[39m=\u001b[39;49mmetric_to_monitor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shahi/OneDrive%20-%20Imperial%20College%20London/Documents/imperial/Dissertation/Notebooks/MyCodes/Marmoset_Residual_Training/test2.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                      bias\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\training\\pytorch_train.py:280\u001b[0m, in \u001b[0;36mrun_pytorch_training\u001b[1;34m(config, model_filename, training_log_filename, verbose, use_multiprocessing, n_workers, model_name, n_gpus, regularized, test_input, metric_to_monitor, model_metrics, bias, pin_memory, amp, prefetch_factor, **unused_args)\u001b[0m\n\u001b[0;32m    271\u001b[0m     validation_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(validation_set,\n\u001b[0;32m    272\u001b[0m                                                     batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mvalidation_batch_size\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    273\u001b[0m                                                     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m                                                     pin_memory\u001b[39m=\u001b[39mpin_memory,\n\u001b[0;32m    277\u001b[0m                                                     prefetch_factor\u001b[39m=\u001b[39mprefetch_factor)\n\u001b[0;32m    279\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m train(model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, criterion\u001b[39m=\u001b[39;49mcriterion, n_epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mn_epochs\u001b[39;49m\u001b[39m\"\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(verbose),\n\u001b[0;32m    281\u001b[0m     training_loader\u001b[39m=\u001b[39;49mtrain_loader, validation_loader\u001b[39m=\u001b[39;49mvalidation_loader, model_filename\u001b[39m=\u001b[39;49mmodel_filename,\n\u001b[0;32m    282\u001b[0m     training_log_filename\u001b[39m=\u001b[39;49mtraining_log_filename,\n\u001b[0;32m    283\u001b[0m     metric_to_monitor\u001b[39m=\u001b[39;49mmetric_to_monitor,\n\u001b[0;32m    284\u001b[0m     early_stopping_patience\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mearly_stopping_patience\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    285\u001b[0m     save_best\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39msave_best\u001b[39;49m\u001b[39m\"\u001b[39;49m, config, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    286\u001b[0m     learning_rate_decay_patience\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mdecay_patience\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    287\u001b[0m     regularized\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mregularized\u001b[39;49m\u001b[39m\"\u001b[39;49m, config, regularized),\n\u001b[0;32m    288\u001b[0m     n_gpus\u001b[39m=\u001b[39;49mn_gpus,\n\u001b[0;32m    289\u001b[0m     vae\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mvae\u001b[39;49m\u001b[39m\"\u001b[39;49m, config, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    290\u001b[0m     decay_factor\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mdecay_factor\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    291\u001b[0m     min_lr\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mmin_learning_rate\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    292\u001b[0m     learning_rate_decay_step_size\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39mdecay_step_size\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    293\u001b[0m     save_every_n_epochs\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39msave_every_n_epochs\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    294\u001b[0m     save_last_n_models\u001b[39m=\u001b[39;49min_config(\u001b[39m\"\u001b[39;49m\u001b[39msave_last_n_models\u001b[39;49m\u001b[39m\"\u001b[39;49m, config),\n\u001b[0;32m    295\u001b[0m     amp\u001b[39m=\u001b[39;49mamp)\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\training\\pytorch_train.py:80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, n_epochs, training_loader, validation_loader, training_log_filename, model_filename, metric_to_monitor, early_stopping_patience, learning_rate_decay_patience, save_best, n_gpus, verbose, regularized, vae, decay_factor, min_lr, learning_rate_decay_step_size, save_every_n_epochs, save_last_n_models, amp)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m loss \u001b[39m=\u001b[39m epoch_training(training_loader, model, criterion, optimizer\u001b[39m=\u001b[39;49moptimizer, epoch\u001b[39m=\u001b[39;49mepoch, n_gpus\u001b[39m=\u001b[39;49mn_gpus,\n\u001b[0;32m     81\u001b[0m                       regularized\u001b[39m=\u001b[39;49mregularized, vae\u001b[39m=\u001b[39;49mvae, scaler\u001b[39m=\u001b[39;49mscaler)\n\u001b[0;32m     82\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\utils\\training_utils\\training_funcs.py:59\u001b[0m, in \u001b[0;36mepoch_training\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, n_gpus, print_frequency, regularized, print_gpu_memory, vae, scaler)\u001b[0m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     58\u001b[0m \u001b[39m# Get the loss and batch size\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m loss, batch_size \u001b[39m=\u001b[39m batch_loss(model, b0, residual, injection_center, criterion, \n\u001b[0;32m     60\u001b[0m                               n_gpus\u001b[39m=\u001b[39;49mn_gpus, regularized\u001b[39m=\u001b[39;49mregularized, vae\u001b[39m=\u001b[39;49mvae, use_amp\u001b[39m=\u001b[39;49muse_amp)\n\u001b[0;32m     61\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\utils\\training_utils\\training_funcs.py:129\u001b[0m, in \u001b[0;36mbatch_loss\u001b[1;34m(model, b0, residual, injection_center, criterion, n_gpus, regularized, vae, use_amp)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m _batch_loss(model, b0, residual, injection_center, criterion, regularized\u001b[39m=\u001b[39mregularized, vae\u001b[39m=\u001b[39mvae)\n\u001b[0;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m _batch_loss(model, b0, residual, injection_center, criterion, regularized\u001b[39m=\u001b[39;49mregularized, vae\u001b[39m=\u001b[39;49mvae)\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\utils\\training_utils\\training_funcs.py:135\u001b[0m, in \u001b[0;36m_batch_loss\u001b[1;34m(model, b0, residual, injection_center, criterion, regularized, vae)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_loss\u001b[39m(model, b0, residual, injection_center, criterion, regularized\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, vae\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m     \u001b[39m# Compute the output\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     output \u001b[39m=\u001b[39m model(b0)\n\u001b[0;32m    137\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(output))\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shahi\\OneDrive - Imperial College London\\Documents\\imperial\\Dissertation\\Notebooks\\MyCodes\\Marmoset_Residual_Training\\models\\model_builders\\encoders.py:208\u001b[0m, in \u001b[0;36mResnetEncoder.forward\u001b[1;34m(self, input_x)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mForward pass\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# Return the model\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_x)\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[0;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[0;32m    610\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 2.00 GiB total capacity; 376.33 MiB already allocated; 1.13 GiB free; 430.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load the configuration\n",
    "configs = load_json(config_path)\n",
    "\n",
    "# Define the metric to monitor based on whether we're skipping val or not\n",
    "if configs[\"skip_val\"]:\n",
    "    metric_to_monitor = \"loss\"\n",
    "else:\n",
    "    metric_to_monitor = \"val_loss\"\n",
    "\n",
    "# Define the groups\n",
    "if configs[\"skip_val\"]:\n",
    "    groups = (\"training\",)\n",
    "else:\n",
    "    groups = (\"training\", \"validation\")\n",
    "\n",
    "model_metrics = (configs[\"evaluation_metric\"],)\n",
    "\n",
    "run_pytorch_training(configs, configs[\"model_filename\"], configs[\"training_log_path\"], \n",
    "                     metric_to_monitor=metric_to_monitor,\n",
    "                     bias=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
