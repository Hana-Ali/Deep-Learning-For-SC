{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "from training import *\n",
    "\n",
    "hpc = False\n",
    "labs = False\n",
    "paperspace = True\n",
    "\n",
    "if hpc:\n",
    "    main_data_path = \"/rds/general/user/hsa22/ephemeral/Brain_MINDS/model_data\"\n",
    "    main_logs_path = \"/rds/general/user/hsa22/ephemeral/Brain_MINDS/predicted_streamlines\"\n",
    "elif labs:\n",
    "    main_data_path = \"/media/hsa22/Expansion/Brain_MINDS/model_data\"\n",
    "    main_logs_path = \"/media/hsa22/Expansion//Brain_MINDS/predicted_streamlines\"\n",
    "elif paperspace:\n",
    "    main_data_path = \"/notebooks/model_data_w_resize\"\n",
    "    main_logs_path = \"/notebooks/predicted_streamlines\"\n",
    "else:\n",
    "    main_data_path = \"D:\\\\Brain-MINDS\\\\model_data\"\n",
    "    main_logs_path = \"D:\\\\Brain-MINDS\\\\predicted_streamlines\"\n",
    "\n",
    "streamline_arrays_path = os.path.join(main_logs_path, \"streamline_predictions\", \"efficientnet\")\n",
    "training_log_folder = os.path.join(main_logs_path, \"training_logs\")\n",
    "model_folder = os.path.join(main_logs_path, \"models\", \"efficientnet\")\n",
    "\n",
    "check_output_folders(streamline_arrays_path, \"streamline arrays\", wipe=False)\n",
    "check_output_folders(training_log_folder, \"training_log_folder\", wipe=False)\n",
    "check_output_folders(model_folder, \"model_folder\", wipe=False)\n",
    "\n",
    "training_log_path = os.path.join(training_log_folder, \"efficientnet.csv\")\n",
    "model_filename = os.path.join(model_folder, \"efficientnet.h5\")\n",
    "\n",
    "# Create the config dictionary\n",
    "config = {\n",
    "\n",
    "    ####### Model #######\n",
    "    \"model_name\" : \"efficientnet\", # Model name\n",
    "    \"input_nc\" : 1,\n",
    "    \"combination\" : True, # Combination\n",
    "    \"task\" : \"classification\", # Task\n",
    "    \"hidden_size\" : 32, # number of neurons\n",
    "    \"depthwise_conv\" : True, # Depthwise convolution\n",
    "    \"library_opt\" : True, # Use stuff from torch_optim\n",
    "\n",
    "    ####### Training #######\n",
    "    \"n_epochs\" : 50, # Number of epochs\n",
    "    \"loss\" : \"negative_log_likelihood_loss\", # Loss function\n",
    "    \"optimizer\" : \"Adam\", # Optimizer\n",
    "    \"evaluation_metric\" : \"negative_log_likelihood_loss\", # Evaluation metric\n",
    "    \"shuffle_dataset\" : True,\n",
    "    \"separate_hemisphere\" : False,\n",
    "    \"cube_size\" : 3, # cube size\n",
    "    \"save_best\" : True, # Save best model\n",
    "    \"overfitting\" : False, # Overfitting\n",
    "\n",
    "    ####### Data #######\n",
    "    \"main_data_path\" : main_data_path, # Data path\n",
    "    \"training_log_path\" : training_log_path, # Training log path\n",
    "    \"model_filename\" : model_filename, # Model filename\n",
    "    \"streamline_arrays_path\" : streamline_arrays_path, # Path to the streamlines array\n",
    "    \"batch_size\" : 8, # Batch size\n",
    "    \"validation_batch_size\" : 8, # Validation batch size\n",
    "    \"num_streamlines\" : 10, # Number of streamlines to consider from each site\n",
    "    \n",
    "    ####### Parameters #######\n",
    "    \"initial_learning_rate\" : 1e-2, # Initial learning rate\n",
    "    \"early_stopping_patience\": None, # Early stopping patience\n",
    "    \"decay_patience\": None, # Learning rate decay patience\n",
    "    \"decay_factor\": None, # Learning rate decay factor\n",
    "    \"min_learning_rate\": 0.1, # Minimum learning rate\n",
    "    \"save_last_n_models\": 10, # Save last n models\n",
    "\n",
    "    ####### Misc #######\n",
    "    \"skip_val\" : False, # Skip validation\n",
    "    \"training_type\" : \"streamline\", # Training type\n",
    "    \"tck_type\" : \"trk\" # TCK type\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StreamlineDataset(main_data_path, num_streamlines=config[\"num_streamlines\"], transforms=None, train=True, tck_type=config[\"tck_type\"],\n",
    "                            task=in_config(\"task\", config, \"classification\"))\n",
    "\n",
    "# Get general parameters\n",
    "n_gpus = config[\"n_gpus\"] if \"n_gpus\" in config else 1\n",
    "n_workers = config[\"n_workers\"] if \"n_workers\" in config else 1\n",
    "pin_memory = config[\"pin_memory\"] if \"pin_memory\" in config else False\n",
    "prefetch_factor = config[\"prefetch_factor\"] if \"prefetch_factor\" in config else 1\n",
    "amp = config[\"amp\"] if \"amp\" in config else False\n",
    "save_best = config[\"save_best\"] if \"save_best\" in config else False\n",
    "save_every_n_epochs = config[\"save_every_n_epochs\"] if \"save_every_n_epochs\" in config else None\n",
    "save_last_n_models = config[\"save_last_n_models\"] if \"save_last_n_models\" in config else None\n",
    "verbose = config[\"verbose\"] if \"verbose\" in config else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default collate\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "collate_fn = default_collate\n",
    "\n",
    "# Define the split size\n",
    "proportions = [.75, .10, .15]\n",
    "lengths = [int(p * len(dataset)) for p in proportions]\n",
    "lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
    "\n",
    "# Split the data\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, lengths, generator=seed)\n",
    "    \n",
    "# Define the training loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                            batch_size=config[\"batch_size\"],\n",
    "                                            shuffle=config[\"shuffle_dataset\"],\n",
    "                                            num_workers=n_workers,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            prefetch_factor=prefetch_factor)\n",
    "\n",
    "# Define the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=config[\"batch_size\"],\n",
    "                                        shuffle=config[\"shuffle_dataset\"],\n",
    "                                        num_workers=n_workers,\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        pin_memory=pin_memory,\n",
    "                                        prefetch_factor=prefetch_factor)\n",
    "\n",
    "# Define the test loader\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                            batch_size=config[\"batch_size\"],\n",
    "                                            shuffle=config[\"shuffle_dataset\"],\n",
    "                                            num_workers=n_workers,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            prefetch_factor=prefetch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the train loader\n",
    "(wmfod, streamlines, labels) = next(iter(train_loader))\n",
    "\n",
    "print(\"wmfod.shape:\", wmfod.shape)\n",
    "print(\"streamlines.shape:\", streamlines.shape)\n",
    "print(\"labels.shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation study between each coordinate in a streamline\n",
    "\n",
    "# Choose which streamline to look at\n",
    "streamline_chosen = 0\n",
    "\n",
    "# Define the indices of the streamlines\n",
    "batch_idx = 0\n",
    "streamline_idx = 1\n",
    "node_idx = 2\n",
    "node_coord_idx = 3\n",
    "\n",
    "# Create list to hold the coordinates of the streamline\n",
    "streamline_coords = []\n",
    "\n",
    "# For every point in the streamline\n",
    "for point in range(streamlines.shape[node_idx] - 1):\n",
    "\n",
    "    # Get the current point from the streamline of all batches\n",
    "    streamline_node = streamlines[:, streamline_chosen, point]\n",
    "\n",
    "    # Get the x, y, z coordinate into a list\n",
    "    curr_coord = np.array([streamline_node[:, 0], streamline_node[:, 1], streamline_node[:, 2]])\n",
    "\n",
    "    # Append the current coordinate to the list of coordinates\n",
    "    streamline_coords.append(curr_coord)\n",
    "\n",
    "# Convert the list of coordinates into a numpy array\n",
    "streamline_coords = np.array(streamline_coords)\n",
    "\n",
    "print(streamline_coords.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
